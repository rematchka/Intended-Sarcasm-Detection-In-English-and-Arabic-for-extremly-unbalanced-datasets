{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"Qarib.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6decc6fa7954404ba5cc69b97ad78a31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_95aeade96bf040d6b1a633124fc775f0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f6d1c81fadee473faa8851ca8e0037ec","IPY_MODEL_a8d7bc66a70246e1ae5714f908de5a80","IPY_MODEL_3b24babc395a414b8b3037df44b1ba9e"]}},"95aeade96bf040d6b1a633124fc775f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f6d1c81fadee473faa8851ca8e0037ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_46ec1e3eba62420998bce468c51617ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_942d786ac8ed4e90a630d749de213750"}},"a8d7bc66a70246e1ae5714f908de5a80":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dd790ad2f5a348719423b6d9bf28f1cf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":656429,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":656429,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d4541df58ab7444787430f3230fc0b90"}},"3b24babc395a414b8b3037df44b1ba9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7632aaf567004fc4bab54d89a925c5cf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 656k/656k [00:00&lt;00:00, 895kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15c9c5399ad04dc5befa251fc4ab52fc"}},"46ec1e3eba62420998bce468c51617ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"942d786ac8ed4e90a630d749de213750":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd790ad2f5a348719423b6d9bf28f1cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d4541df58ab7444787430f3230fc0b90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7632aaf567004fc4bab54d89a925c5cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"15c9c5399ad04dc5befa251fc4ab52fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c05b286a0ae4e339d70a6e5ac40b346":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_82024c735c914393b1e217a683b33d5c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_08c5afb0e4c841dcb95308d546a14e0d","IPY_MODEL_6070d13464de4ee194394aa2b896ac22","IPY_MODEL_b648c272ad5d4e82b4da3e20396887df"]}},"82024c735c914393b1e217a683b33d5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08c5afb0e4c841dcb95308d546a14e0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_614cdf27e0d94424ab8099c044833173","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_916c70fdcd134f37b283e2f75441a424"}},"6070d13464de4ee194394aa2b896ac22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ada9d34a77c94d36a3a35af199779091","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":504,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":504,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_56aed65636494fbbb9c2167f4b22ee0d"}},"b648c272ad5d4e82b4da3e20396887df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_20f524bc94fb43b0a8e4771008f8dca5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 504/504 [00:00&lt;00:00, 15.3kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38f74123baae44a390dc3dbbbd554ae1"}},"614cdf27e0d94424ab8099c044833173":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"916c70fdcd134f37b283e2f75441a424":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ada9d34a77c94d36a3a35af199779091":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"56aed65636494fbbb9c2167f4b22ee0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"20f524bc94fb43b0a8e4771008f8dca5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"38f74123baae44a390dc3dbbbd554ae1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ea6505990784723a6bd53af02cacbfc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f560b4c737b24f4994b3c7c202dc1510","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_979417964761468d89bcf0c757b3c15b","IPY_MODEL_fcbd8696565e4ddcb163ca0d4085b2fb","IPY_MODEL_bd332878bac44777abaedeee965f8b82"]}},"f560b4c737b24f4994b3c7c202dc1510":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"979417964761468d89bcf0c757b3c15b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b8738337c0a34941b7f9a8ece71bf1d3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_427adc07874a40efb4abaf95cea357b0"}},"fcbd8696565e4ddcb163ca0d4085b2fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4dc01c17a21c493e8d88149b3a5b1a9c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":543488365,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":543488365,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_be84719dd2f14e67a24d3726b2dbb916"}},"bd332878bac44777abaedeee965f8b82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ff43993b0ae0466ba3f874dbb65f192a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 543M/543M [00:16&lt;00:00, 52.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0683c86e1040495dac9e74d8620274bd"}},"b8738337c0a34941b7f9a8ece71bf1d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"427adc07874a40efb4abaf95cea357b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4dc01c17a21c493e8d88149b3a5b1a9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"be84719dd2f14e67a24d3726b2dbb916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ff43993b0ae0466ba3f874dbb65f192a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0683c86e1040495dac9e74d8620274bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"JUiwkRv-XAkW"},"source":["# Download MARBERT checkpoint"]},{"cell_type":"code","metadata":{"id":"_8wBRd5zXAkX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5283418-f17c-41cb-fabf-d129e8ed86b6","executionInfo":{"status":"ok","timestamp":1642792988398,"user_tz":-120,"elapsed":12425,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-01-21 19:22:56--  https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz\n","Resolving huggingface.co (huggingface.co)... 54.205.126.93, 34.204.221.201, 34.224.55.150, ...\n","Connecting to huggingface.co (huggingface.co)|54.205.126.93|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc [following]\n","--2022-01-21 19:22:56--  https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc\n","Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.84.158.23, 52.84.158.37, 52.84.158.11, ...\n","Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.84.158.23|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 607066087 (579M) [application/x-gzip]\n","Saving to: ‘MARBERT_pytorch_verison.tar.gz’\n","\n","MARBERT_pytorch_ver 100%[===================>] 578.94M  55.6MB/s    in 11s     \n","\n","2022-01-21 19:23:08 (51.6 MB/s) - ‘MARBERT_pytorch_verison.tar.gz’ saved [607066087/607066087]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"9FYvgJevXAkY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2649ed05-d2b3-4458-c332-af5d39b0c4be","executionInfo":{"status":"ok","timestamp":1642792995879,"user_tz":-120,"elapsed":7485,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["!tar -xvf MARBERT_pytorch_verison.tar.gz"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["MARBERT_pytorch_verison/\n","MARBERT_pytorch_verison/pytorch_model.bin\n","MARBERT_pytorch_verison/config.json\n","MARBERT_pytorch_verison/vocab.txt\n"]}]},{"cell_type":"code","metadata":{"id":"C2pkLcyfXAka","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b2ee39a-0266-4546-8733-959604da06b7","executionInfo":{"status":"ok","timestamp":1642793006347,"user_tz":-120,"elapsed":10492,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["!pip install GPUtil pytorch_pretrained_bert==0.5.0 transformers==4.3.0"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting GPUtil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Collecting pytorch_pretrained_bert==0.5.0\n","  Downloading pytorch_pretrained_bert-0.5.0-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 4.8 MB/s \n","\u001b[?25hCollecting transformers==4.3.0\n","  Downloading transformers-4.3.0-py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 14.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (1.19.5)\n","Collecting boto3\n","  Downloading boto3-1.20.40-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 74.8 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (1.10.0+cu111)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (4.62.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 66.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (4.10.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 56.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (3.4.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.5.0) (3.10.0.2)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.24.0,>=1.23.40\n","  Downloading botocore-1.23.40-py3-none-any.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 62.6 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 10.1 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 69.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.40->boto3->pytorch_pretrained_bert==0.5.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.40->boto3->pytorch_pretrained_bert==0.5.0) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.0) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.0) (3.0.6)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert==0.5.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert==0.5.0) (2021.10.8)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 79.1 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert==0.5.0) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0) (1.1.0)\n","Building wheels for collected packages: GPUtil\n","  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=e8ce0e28d685b35c385834d47ed0c4ff78a31bab9406acb55ce73ba3f93b99ab\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","Successfully built GPUtil\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sacremoses, boto3, transformers, pytorch-pretrained-bert, GPUtil\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed GPUtil-1.4.0 boto3-1.20.40 botocore-1.23.40 jmespath-0.10.0 pytorch-pretrained-bert-0.5.0 s3transfer-0.5.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.3.0 urllib3-1.25.11\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgdLbPEQma7k","outputId":"19706368-ba26-4d8d-ae60-b92c7266e424","executionInfo":{"status":"ok","timestamp":1642793025854,"user_tz":-120,"elapsed":19514,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"6xBm7opGXAkb"},"source":["# Fine-tuning code"]},{"cell_type":"code","metadata":{"id":"9oOXwzeXXAkb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fdfa8370-1281-46cf-c44c-b99d7830e8b3","executionInfo":{"status":"ok","timestamp":1642793033994,"user_tz":-120,"elapsed":8145,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["# (1)load libraries \n","import json, sys, regex\n","import torch\n","import GPUtil\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import os\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n","##----------------------------------------------------\n","from transformers import *\n","from transformers import XLMRobertaConfig\n","from transformers import XLMRobertaModel\n","from transformers import AutoTokenizer, AutoModelWithLMHead\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n","from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import AutoTokenizer, AutoModel\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"]}]},{"cell_type":"code","metadata":{"id":"QnNrYLjBuLFN","executionInfo":{"status":"ok","timestamp":1642793033995,"user_tz":-120,"elapsed":10,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["import re"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQWKMrXPXAkc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a47b8745-ce73-445c-9a54-f80ca35e06d6","executionInfo":{"status":"ok","timestamp":1642793033995,"user_tz":-120,"elapsed":9,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (\"your device \", device)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["your device  cuda\n"]}]},{"cell_type":"code","metadata":{"id":"KKujHwr5XAkd","executionInfo":{"status":"ok","timestamp":1642793033996,"user_tz":-120,"elapsed":9,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["def create_label2ind_file(file, label_col):\n","    labels_json = {}\n","\n","    # load train_dev_test file\n","\n","    df = pd.read_csv(file)\n","    \n","    # get labels and sort it A-Z\n","\n","    labels = df[label_col].unique().astype(np.float)\n","    labels.sort()\n","\n","    # convert labels to indexes\n","\n","    for idx in range(0, len(labels)):\n","        labels_json[labels[idx]] = idx\n","    with open(label2idx_file, 'w') as json_file:\n","        json.dump(labels_json, json_file)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNZxr8m6XAkd","executionInfo":{"status":"ok","timestamp":1642793033996,"user_tz":-120,"elapsed":9,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["#!/usr/bin/python\n","# -*- coding: utf-8 -*-\n","\n","\n","def data_prepare_BERT(\n","    file_path,\n","    lab2ind,\n","    tokenizer,\n","    content_col,\n","    label_col,\n","    MAX_LEN,\n","    ):\n","\n","    # Use pandas to load dataset\n","\n","    df = pd.read_csv(file_path)\n","\n","    df.dropna(inplace=True)\n","    df = df[df[content_col].notnull()]\n","    df = df[df[label_col].notnull()]\n","\n","\n","  # df[content_col] = df.tweet.apply(clean_text)\n","\n","    print ('Data size ', df.shape)\n","\n","    # Create sentence and label lists\n","\n","    sentences = df[content_col].values\n","    sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in\n","                 sentences]\n","    print ('The first sentence:')\n","    print (sentences[0])\n","\n","    # Create sentence and label lists\n","\n","    labels = df[label_col].values\n","\n","    # print (labels)\n","\n","    # labels = [lab2ind[i] for i in labels]\n","\n","    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n","\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    print ('Tokenize the first sentence:')\n","    print (tokenized_texts[0])\n","\n","    # print(\"Label is \", labels[0])\n","    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in\n","                 tokenized_texts]\n","    print ('Index numbers of the first sentence:')\n","    print (input_ids[0])\n","\n","    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n","    # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n","    input_ids = pad_sequences(\n","        input_ids,\n","        maxlen=MAX_LEN + 2,\n","        dtype='long',\n","        truncating='post',\n","        padding='post',\n","        value=pad_ind,\n","        )\n","    print ('Index numbers of the first sentence after padding:\\n',\n","           input_ids[0])\n","\n","    # Create attention masks\n","\n","    attention_masks = []\n","\n","    # Create a mask of 1s for each token followed by 0s for padding\n","\n","    for seq in input_ids:\n","        seq_mask = [float(i > 0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # Convert all of our data into torch tensors, the required datatype for our model\n","\n","    inputs = torch.tensor(input_ids)\n","    labels = torch.tensor(labels)\n","    masks = torch.tensor(attention_masks)\n","    return inputs, labels, masks\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"wxIYQznY1r9F","executionInfo":{"status":"ok","timestamp":1642793033997,"user_tz":-120,"elapsed":9,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["#!/usr/bin/python\n","# -*- coding: utf-8 -*-\n","\n","\n","def data_prepare_BERT_test(\n","    file_path,\n","    lab2ind,\n","    tokenizer,\n","    content_col,\n","    \n","    MAX_LEN,\n","    ):\n","\n","    # Use pandas to load dataset\n","\n","    df = pd.read_csv(file_path)\n","\n","    df.dropna(inplace=True)\n","    df = df[df[content_col].notnull()]\n","    df = df[df[label_col].notnull()]\n","    \n","\n","  # df[content_col] = df.tweet.apply(clean_text)\n","\n","    print ('Data size ', df.shape)\n","\n","    # Create sentence and label lists\n","\n","    sentences = df[content_col].values\n","    sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in\n","                 sentences]\n","    print ('The first sentence:')\n","    print (sentences[0])\n","\n","    # Create sentence and label lists\n","\n","   \n","\n","    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n","\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    print ('Tokenize the first sentence:')\n","    print (tokenized_texts[0])\n","\n","    # print(\"Label is \", labels[0])\n","    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in\n","                 tokenized_texts]\n","    print ('Index numbers of the first sentence:')\n","    print (input_ids[0])\n","\n","    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n","    # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n","    input_ids = pad_sequences(\n","        input_ids,\n","        maxlen=MAX_LEN + 2,\n","        dtype='long',\n","        truncating='post',\n","        padding='post',\n","        value=pad_ind,\n","        )\n","    print ('Index numbers of the first sentence after padding:\\n',\n","           input_ids[0])\n","\n","    # Create attention masks\n","\n","    attention_masks = []\n","\n","    # Create a mask of 1s for each token followed by 0s for padding\n","\n","    for seq in input_ids:\n","        seq_mask = [float(i > 0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # Convert all of our data into torch tensors, the required datatype for our model\n","\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","    return inputs,  masks\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdPCPv8VXAke","executionInfo":{"status":"ok","timestamp":1642793033997,"user_tz":-120,"elapsed":9,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","# def flat_accuracy(preds, labels):\n","#\t  pred_flat = np.argmax(preds, axis=1).flatten()\n","#\t  labels_flat = labels.flatten()\n","#\t  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","def flat_pred(preds, labels):\n","\tpred_flat = np.argmax(preds, axis=1).flatten()\n","\tlabels_flat = labels.flatten()\n","\treturn pred_flat.tolist(), labels_flat.tolist()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"5vyvc8JoXAke","executionInfo":{"status":"ok","timestamp":1642793034400,"user_tz":-120,"elapsed":10,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["\n","def train(model, iterator, optimizer, scheduler, criterion):\n","\t\n","\tmodel.train()\n","\tepoch_loss = 0\n","\tfor i, batch in enumerate(iterator):\n","\t\t# Add batch to GPU\n","\t\tbatch = tuple(t.to(device) for t in batch)\n","\t\t# Unpack the inputs from our dataloader\n","\t\tinput_ids, input_mask, labels = batch\n","\t\toutputs = model(input_ids, input_mask, labels=labels)\n","\t\tloss, logits = outputs[:2]\n","\t\t# delete used variables to free GPU memory\n","\t\tdel batch, input_ids, input_mask, labels\n","\t\toptimizer.zero_grad()\n","\t\tif torch.cuda.device_count() == 1:\n","\t\t\tloss.backward()\n","\t\t\tepoch_loss += loss.cpu().item()\n","\t\telse:\n","\t\t\tloss.sum().backward()\n","\t\t\tepoch_loss += loss.sum().cpu().item()\n","\t\toptimizer.step()\n","\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n","\t\t# optimizer.step()\n","\t\tscheduler.step()\n","\t# free GPU memory\n","\tif device == 'cuda':\n","\t\ttorch.cuda.empty_cache()\n","\treturn epoch_loss / len(iterator)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hjwa6a3bXAkf","executionInfo":{"status":"ok","timestamp":1642793034401,"user_tz":-120,"elapsed":9,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["\n","def evaluate(model, iterator, criterion):\n","\tmodel.eval()\n","\tepoch_loss = 0\n","\tall_pred=[]\n","\tall_label = []\n","\twith torch.no_grad():\n","\t\tfor i, batch in enumerate(iterator):\n","\t\t\t# Add batch to GPU\n","\t\t\tbatch = tuple(t.to(device) for t in batch)\n","\t\t\t# Unpack the inputs from our dataloader\n","\t\t\tinput_ids, input_mask, labels = batch\n","\t\t\toutputs = model(input_ids, input_mask, labels=labels)\n","\t\t\tloss, logits = outputs[:2]\n","\t\t\t# delete used variables to free GPU memory\n","\t\t\tdel batch, input_ids, input_mask\n","\t\t\tif torch.cuda.device_count() == 1:\n","\t\t\t\tepoch_loss += loss.cpu().item()\n","\t\t\telse:\n","\t\t\t\tepoch_loss += loss.sum().cpu().item()\n","\t\t\t# identify the predicted class for each example in the batch\n","\t\t\tprobabilities, predicted = torch.max(logits.cpu().data, 1)\n","\t\t\t# put all the true labels and predictions to two lists\n","\t\t\tall_pred.extend(predicted)\n","\t\t\tall_label.extend(labels.cpu())\n","\taccuracy = accuracy_score(all_label, all_pred)\n","\tf1score = f1_score(all_label, all_pred, average='macro') \n","\trecall = recall_score(all_label, all_pred, average='macro')\n","\tprecision = precision_score(all_label, all_pred, average='macro')\n","\treport = classification_report(all_label, all_pred)\n","\treturn (epoch_loss / len(iterator)), accuracy, f1score, recall, precision\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nWmg6P-XAkf","executionInfo":{"status":"ok","timestamp":1642793034402,"user_tz":-120,"elapsed":9,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["\n","def fine_tuning(config):\n","\t#---------------------------------------\n","\tprint (\"[INFO] step (1) load train_test config file\")\n","\t# config_file = open(config_file, 'r', encoding=\"utf8\")\n","\t# config = json.load(config_file)\n","\ttask_name = config[\"task_name\"]\n","\tcontent_col = config[\"content_col\"]\n","\tlabel_col = config[\"label_col\"]\n","\ttrain_file = config[\"data_dir\"]+config[\"train_file\"]\n","\tdev_file = config[\"data_dir\"]+config[\"dev_file\"]\n","\tsortby = config[\"sortby\"]\n","\tmax_seq_length= int(config[\"max_seq_length\"])\n","\tbatch_size = int(config[\"batch_size\"])\n","\tlr_var = float(config[\"lr\"])\n","\tmodel_path = config['pretrained_model_path']\n","\tnum_epochs = config['epochs'] # Number of training epochs (authors recommend between 2 and 4)\n","\tglobal label2idx_file\n","\tlabel2idx_file = config[\"data_dir\"]+config[\"task_name\"]+\"_labels-dict.json\"\n","\t#-------------------------------------------------------\n","\tprint (\"[INFO] step (2) convert labels2index\")\n","\tcreate_label2ind_file(train_file, label_col)\n","\tprint (label2idx_file)\n","\t#---------------------------------------------------------\n","\tprint (\"[INFO] step (3) check checkpoit directory and report file\")\n","\tckpt_dir = config[\"data_dir\"]+task_name+\"_bert_ckpt/\"\n","\treport = ckpt_dir+task_name+\"_report.tsv\"\n","\tsorted_report = ckpt_dir+task_name+\"_report_sorted.tsv\"\n","\tif not os.path.exists(ckpt_dir):\n","\t\tos.mkdir(ckpt_dir)\n","\t#-------------------------------------------------------\n","\tprint (\"[INFO] step (4) load label to number dictionary\")\n","\tlab2ind = json.load(open(label2idx_file))\n","\tprint (\"[INFO] train_file\", train_file)\n","\tprint (\"[INFO] dev_file\", dev_file)\n","\tprint (\"[INFO] num_epochs\", num_epochs)\n","\tprint (\"[INFO] model_path\", model_path)\n","\tprint (\"max_seq_length\", max_seq_length, \"batch_size\", batch_size)\n","\t#-------------------------------------------------------\n","\tprint (\"[INFO] step (5) Use defined funtion to extract tokanize data\")\n","\t# tokenizer from pre-trained BERT model\n","\tprint (\"loading BERT setting\")\n","\ttokenizer = BertTokenizer.from_pretrained(model_path)\n","\ttrain_inputs, train_labels, train_masks = data_prepare_BERT(train_file, lab2ind, tokenizer,content_col, label_col, max_seq_length)\n","\tvalidation_inputs, validation_labels, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)\n","\t# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n","\tmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n","\t#--------------------------------------\n","\tprint (\"[INFO] step (6) Create an iterator of data with torch DataLoader.\")\n","#\t\t  This helps save on memory during training because, unlike a for loop,\\\n","#\t\t  with an iterator the entire dataset does not need to be loaded into memory\")\n","\ttrain_data = TensorDataset(train_inputs, train_masks, train_labels)\n","\ttrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n"," \n","\t#---------------------------\n","\tvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","\tvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n","\t#------------------------------------------\n","\tprint (\"[INFO] step (7) run with parallel GPUs\")\n","\tif torch.cuda.is_available():\n","\t\tif torch.cuda.device_count() == 1:\n","\t\t\tprint(\"Run\", \"with one GPU\")\n","\t\t\tmodel = model.to(device)\n","\t\telse:\n","\t\t\tn_gpu = torch.cuda.device_count()\n","\t\t\tprint(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n","\t\t\tdevice_ids = GPUtil.getAvailable(limit = 4)\n","\t\t\ttorch.backends.cudnn.benchmark = True\n","\t\t\tmodel = model.to(device)\n","\t\t\tmodel = nn.DataParallel(model, device_ids=device_ids)\n","\telse:\n","\t\tprint(\"Run\", \"with CPU\")\n","\t\tmodel = model\n","\t#---------------------------------------------------\n","\tprint (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n","\tglobal max_grad_norm\n","\tmax_grad_norm = 1.0\n","\twarmup_proportion = 0.1\n","\tnum_training_steps\t= len(train_dataloader) * num_epochs\n","\tnum_warmup_steps = num_training_steps * warmup_proportion\n","\t### In Transformers, optimizer and schedules are instantiated like this:\n","\t# Note: AdamW is a class from the huggingface library\n","\t# the 'W' stands for 'Weight Decay\"\n","\toptimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False)\n","\t# schedules\n","\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n","\t# We use nn.CrossEntropyLoss() as our loss function. \n","\tcriterion = nn.CrossEntropyLoss()\n","\t#---------------------------------------------------\n","\tprint (\"[INFO] step (9) start fine_tuning\")\n","\tfor epoch in trange(num_epochs, desc=\"Epoch\"):\n","\t\ttrain_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\t  \n","\t\tval_loss, val_acc, val_f1, val_recall, val_precision = evaluate(model, validation_dataloader, criterion)\n","# \t\tprint (train_loss, val_acc)\n","\t\t# Create checkpoint at end of each epoch\n","\t\tif not os.path.exists(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/'): os.mkdir(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/')\n","\t\tmodel.save_pretrained(ckpt_dir+ 'model_' + str(int(epoch + 1)) + '/')\n","\t\tepoch_eval_results = {\"epoch_num\":int(epoch + 1),\"train_loss\":train_loss,\n","\t\t\t\t\t  \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1,\"lr\":lr_var }\n","\t\twith open(report,\"a\") as fOut:\n","\t\t\tfOut.write(json.dumps(epoch_eval_results)+\"\\n\")\n","\t\t\tfOut.flush()\n","\t\t#------------------------------------\n","\t\treport_df = pd.read_json(report, orient='records', lines=True)\n","\t\treport_df.sort_values(by=[sortby],ascending=False, inplace=True)\n","\t\treport_df.to_csv(sorted_report,sep=\"\\t\",index=False)\n","\treturn report_df"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDnDyOFHiZvk"},"source":["# Run fine-tuning for 5 epochs"]},{"cell_type":"code","metadata":{"id":"Ol6LaSAAXAki","executionInfo":{"status":"ok","timestamp":1642793950308,"user_tz":-120,"elapsed":271,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["\n","config={\"task_name\": \"ISARCASM_Qarib_bert_base_9920k\", #output directory name\n","             \"data_dir\": \"/content/drive/MyDrive/ISarcasm/Splitted_En_dataset/\", #data directory\n","             \"train_file\": \"tr.Ar.csv\", #train file path\n","             \"dev_file\": \"valid.Ar.csv\", #dev file path or test file path\n","             \"pretrained_model_path\": 'qarib/bert-base-qarib_far_9920k', #MARBERT checkpoint path\n","             \"epochs\": 5, #number of epochs\n","             \"content_col\": \"tweet\", #text column\n","             \"label_col\": \"sarcastic\", #label column\n","             \"lr\": 2e-06, #learning rate\n","              \"max_seq_length\": 256, #max sequance length\n","              \"batch_size\": 16, #batch shize\n","              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"jq9kOFoaXAkj","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6decc6fa7954404ba5cc69b97ad78a31","95aeade96bf040d6b1a633124fc775f0","f6d1c81fadee473faa8851ca8e0037ec","a8d7bc66a70246e1ae5714f908de5a80","3b24babc395a414b8b3037df44b1ba9e","46ec1e3eba62420998bce468c51617ae","942d786ac8ed4e90a630d749de213750","dd790ad2f5a348719423b6d9bf28f1cf","d4541df58ab7444787430f3230fc0b90","7632aaf567004fc4bab54d89a925c5cf","15c9c5399ad04dc5befa251fc4ab52fc","0c05b286a0ae4e339d70a6e5ac40b346","82024c735c914393b1e217a683b33d5c","08c5afb0e4c841dcb95308d546a14e0d","6070d13464de4ee194394aa2b896ac22","b648c272ad5d4e82b4da3e20396887df","614cdf27e0d94424ab8099c044833173","916c70fdcd134f37b283e2f75441a424","ada9d34a77c94d36a3a35af199779091","56aed65636494fbbb9c2167f4b22ee0d","20f524bc94fb43b0a8e4771008f8dca5","38f74123baae44a390dc3dbbbd554ae1","3ea6505990784723a6bd53af02cacbfc","f560b4c737b24f4994b3c7c202dc1510","979417964761468d89bcf0c757b3c15b","fcbd8696565e4ddcb163ca0d4085b2fb","bd332878bac44777abaedeee965f8b82","b8738337c0a34941b7f9a8ece71bf1d3","427adc07874a40efb4abaf95cea357b0","4dc01c17a21c493e8d88149b3a5b1a9c","be84719dd2f14e67a24d3726b2dbb916","ff43993b0ae0466ba3f874dbb65f192a","0683c86e1040495dac9e74d8620274bd"]},"outputId":"9bf2d727-cc68-4b6e-a99d-98607edd2f89","executionInfo":{"status":"ok","timestamp":1642794478166,"user_tz":-120,"elapsed":526520,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["report_df = fine_tuning(config)"],"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] step (1) load train_test config file\n","[INFO] step (2) convert labels2index\n","/content/drive/MyDrive/ISarcasm/Splitted_En_dataset/ISARCASM_Qarib_bert_base_9920k_labels-dict.json\n","[INFO] step (3) check checkpoit directory and report file\n","[INFO] step (4) load label to number dictionary\n","[INFO] train_file /content/drive/MyDrive/ISarcasm/Splitted_En_dataset/tr.Ar.csv\n","[INFO] dev_file /content/drive/MyDrive/ISarcasm/Splitted_En_dataset/valid.Ar.csv\n","[INFO] num_epochs 5\n","[INFO] model_path qarib/bert-base-qarib_far_9920k\n","max_seq_length 256 batch_size 16\n","[INFO] step (5) Use defined funtion to extract tokanize data\n","loading BERT setting\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6decc6fa7954404ba5cc69b97ad78a31","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/656k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Data size  (1861, 3)\n","The first sentence:\n","[CLS] من تونس  سوريا فى القلب [SEP]\n","Tokenize the first sentence:\n","['[CLS]', 'من', 'تونس', 'سوريا', 'فى', 'ال', '##قلب', '[SEP]']\n","Index numbers of the first sentence:\n","[2, 19, 1865, 251, 75, 7, 9301, 3]\n","Index numbers of the first sentence after padding:\n"," [   2   19 1865  251   75    7 9301    3    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0]\n","Data size  (620, 3)\n","The first sentence:\n","[CLS] من طلب العلا شافلة واسطه أما سهر الليالي ده فيلم تشوفه مع امك [SEP]\n","Tokenize the first sentence:\n","['[CLS]', 'من', 'طلب', 'ال', '##عل', '##ا', 'شاف', '##ل', '##ة', 'واسطه', 'اما', 'سهر', 'الليالي', 'ده', 'فيلم', 'تشوف', '##ه', 'مع', 'امك', '[SEP]']\n","Index numbers of the first sentence:\n","[2, 19, 318, 7, 10786, 16, 2097, 149, 9, 27762, 428, 1853, 4776, 413, 995, 1038, 11, 51, 1669, 3]\n","Index numbers of the first sentence after padding:\n"," [    2    19   318     7 10786    16  2097   149     9 27762   428  1853\n","  4776   413   995  1038    11    51  1669     3     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0]\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c05b286a0ae4e339d70a6e5ac40b346","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/504 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ea6505990784723a6bd53af02cacbfc","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/543M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at qarib/bert-base-qarib_far_9920k were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at qarib/bert-base-qarib_far_9920k and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] step (6) Create an iterator of data with torch DataLoader.\n","[INFO] step (7) run with parallel GPUs\n","Run with one GPU\n","[INFO] step (8) set Parameters, schedules, and loss function\n","[INFO] step (9) start fine_tuning\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 5/5 [08:20<00:00, 100.11s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"5V9ZOdVNXAkk","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"3b85c761-fc41-409e-ad26-34aa87a4c504","executionInfo":{"status":"ok","timestamp":1642794478167,"user_tz":-120,"elapsed":24,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["report_df.head(5)"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-9df910ab-af61-45ce-a149-6415a5d7864c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch_num</th>\n","      <th>train_loss</th>\n","      <th>val_acc</th>\n","      <th>val_recall</th>\n","      <th>val_precision</th>\n","      <th>val_f1</th>\n","      <th>lr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.136870</td>\n","      <td>0.893548</td>\n","      <td>0.843778</td>\n","      <td>0.866610</td>\n","      <td>0.854230</td>\n","      <td>0.000002</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.216248</td>\n","      <td>0.890323</td>\n","      <td>0.848005</td>\n","      <td>0.857305</td>\n","      <td>0.852486</td>\n","      <td>0.000002</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.163630</td>\n","      <td>0.890323</td>\n","      <td>0.837367</td>\n","      <td>0.863393</td>\n","      <td>0.849118</td>\n","      <td>0.000002</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.297555</td>\n","      <td>0.882258</td>\n","      <td>0.855382</td>\n","      <td>0.840035</td>\n","      <td>0.847193</td>\n","      <td>0.000002</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.512378</td>\n","      <td>0.838710</td>\n","      <td>0.819905</td>\n","      <td>0.785116</td>\n","      <td>0.798848</td>\n","      <td>0.000002</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9df910ab-af61-45ce-a149-6415a5d7864c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9df910ab-af61-45ce-a149-6415a5d7864c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9df910ab-af61-45ce-a149-6415a5d7864c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   epoch_num  train_loss   val_acc  ...  val_precision    val_f1        lr\n","4          5    0.136870  0.893548  ...       0.866610  0.854230  0.000002\n","2          3    0.216248  0.890323  ...       0.857305  0.852486  0.000002\n","3          4    0.163630  0.890323  ...       0.863393  0.849118  0.000002\n","1          2    0.297555  0.882258  ...       0.840035  0.847193  0.000002\n","0          1    0.512378  0.838710  ...       0.785116  0.798848  0.000002\n","\n","[5 rows x 7 columns]"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"yLHOqQBt3TLN","executionInfo":{"status":"ok","timestamp":1642794478969,"user_tz":-120,"elapsed":3,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["lab2ind = json.load(open('/content/drive/MyDrive/ISarcasm/Splitted_En_dataset/ISARCASM_Qarib_bert_base_9920k_labels-dict.json'))"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoRMJ72mhK9W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"efe4e46f-57ca-402f-98ef-2793cb9ae8ac","executionInfo":{"status":"ok","timestamp":1642794485538,"user_tz":-120,"elapsed":6572,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["tokenizer = BertTokenizer.from_pretrained('qarib/bert-base-qarib_far_9920k')\n","max_seq_length=256\n","train_inputs, train_labels, train_masks = data_prepare_BERT('/content/drive/MyDrive/ISarcasm/Splitted_En_dataset/tr.Ar.csv', lab2ind, tokenizer,'tweet', \"sarcastic\", max_seq_length)\n","test_inputs, test_labels, test_masks = data_prepare_BERT('/content/drive/MyDrive/ISarcasm/Splitted_En_dataset/test.Ar.csv', lab2ind, tokenizer, 'tweet','sarcastic', max_seq_length)\n","# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n","model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/ISarcasm/Splitted_En_dataset/ISARCASM_Qarib_bert_base_9920k_bert_ckpt/model_5', num_labels=len(lab2ind))"],"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Data size  (1861, 3)\n","The first sentence:\n","[CLS] من تونس  سوريا فى القلب [SEP]\n","Tokenize the first sentence:\n","['[CLS]', 'من', 'تونس', 'سوريا', 'فى', 'ال', '##قلب', '[SEP]']\n","Index numbers of the first sentence:\n","[2, 19, 1865, 251, 75, 7, 9301, 3]\n","Index numbers of the first sentence after padding:\n"," [   2   19 1865  251   75    7 9301    3    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0]\n","Data size  (621, 3)\n","The first sentence:\n","[CLS] ظهرت معزاز كبير [SEP]\n","Tokenize the first sentence:\n","['[CLS]', 'ظهر', '##ت', 'معز', '##از', 'كبير', '[SEP]']\n","Index numbers of the first sentence:\n","[2, 711, 15, 6632, 7550, 183, 3]\n","Index numbers of the first sentence after padding:\n"," [   2  711   15 6632 7550  183    3    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0]\n"]}]},{"cell_type":"code","metadata":{"id":"W1ce7uen1Qpy","executionInfo":{"status":"ok","timestamp":1642794485538,"user_tz":-120,"elapsed":16,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=16)"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"aRAB4mCZ2M3r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80055f71-61d8-4b9d-d16f-1c1a36a2e896","executionInfo":{"status":"ok","timestamp":1642794485539,"user_tz":-120,"elapsed":16,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["if torch.cuda.is_available():\n","\t\tif torch.cuda.device_count() == 1:\n","\t\t\tprint(\"Run\", \"with one GPU\")\n","\t\t\tmodel = model.to(device)\n","\t\telse:\n","\t\t\tn_gpu = torch.cuda.device_count()\n","\t\t\tprint(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n","\t\t\tdevice_ids = GPUtil.getAvailable(limit = 4)\n","\t\t\ttorch.backends.cudnn.benchmark = True\n","\t\t\tmodel = model.to(device)\n","\t\t\tmodel = nn.DataParallel(model, device_ids=device_ids)\n","else:\n","  print(\"Run\", \"with CPU\")\n","  model = model"],"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Run with one GPU\n"]}]},{"cell_type":"code","metadata":{"id":"qT59ESky4L93","executionInfo":{"status":"ok","timestamp":1642794485539,"user_tz":-120,"elapsed":15,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["criterion = nn.CrossEntropyLoss()"],"execution_count":80,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DL9tEe92RC5","executionInfo":{"status":"ok","timestamp":1642794496735,"user_tz":-120,"elapsed":11211,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["model.eval()\n","all_pred=[]\n","all_label = []\n","with torch.no_grad():\n","  for i, batch in enumerate(test_dataloader):\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    input_ids, input_mask, labels = batch\n","    outputs = model(input_ids, input_mask, labels=labels)\n","    loss, logits = outputs[:2]\n","    \n","    probabilities, predicted = torch.max(logits.cpu().data, 1)\n","    # put all the true labels and predictions to two lists\n","    all_pred.extend(predicted)\n","    all_label.extend(labels.cpu())\n","accuracy = accuracy_score(all_label, all_pred)\n","macro_f1_pos_neg = f1_score(all_label, all_pred,average='macro',labels=[0,1])\n","f1score = f1_score(all_label, all_pred, average='macro') \n","recall = recall_score(all_label, all_pred, average='macro')\n","precision = precision_score(all_label, all_pred, average='macro')\n","report = classification_report(all_label, all_pred)"],"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":["qarib/bert-base-qarib_far_9920k"],"metadata":{"id":"9OqxlnGqJ0Ng"}},{"cell_type":"code","source":["print(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRtfSxw-Jwf2","executionInfo":{"status":"ok","timestamp":1642794534267,"user_tz":-120,"elapsed":266,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}},"outputId":"2a08c3ff-b883-4089-a5b4-c297193d8ef0"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93       486\n","           1       0.75      0.74      0.74       135\n","\n","    accuracy                           0.89       621\n","   macro avg       0.84      0.84      0.84       621\n","weighted avg       0.89      0.89      0.89       621\n","\n"]}]},{"cell_type":"markdown","source":["Qarib bert base"],"metadata":{"id":"M9K7DdH1Jycl"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sV5f1UfqfJ25","outputId":"3522893c-f73f-4bbf-af77-e95685e61d80","executionInfo":{"status":"ok","timestamp":1642793880839,"user_tz":-120,"elapsed":23,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["print(report)"],"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.94      0.94       486\n","           1       0.78      0.80      0.79       135\n","\n","    accuracy                           0.91       621\n","   macro avg       0.86      0.87      0.87       621\n","weighted avg       0.91      0.91      0.91       621\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJu0tXU3feWW","outputId":"ae4716cb-5750-438f-f462-a87d2ccb9f0a","executionInfo":{"status":"ok","timestamp":1642793880839,"user_tz":-120,"elapsed":21,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["accuracy"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9082125603864735"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6pTXudFRff7c","outputId":"3871d047-d3fc-4fac-dde8-d9e0513f222a","executionInfo":{"status":"ok","timestamp":1642793880839,"user_tz":-120,"elapsed":19,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["macro_f1_pos_neg"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8661926308985133"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a515-AFkfgyX","outputId":"cc5e96f3-ebd6-4876-b603-54d6e59b466f","executionInfo":{"status":"ok","timestamp":1642793880840,"user_tz":-120,"elapsed":19,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["f1score"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8661926308985133"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6dAR2e7fiKL","outputId":"75a6058e-d644-4cba-d760-607720b1c3a0","executionInfo":{"status":"ok","timestamp":1642793880840,"user_tz":-120,"elapsed":17,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["recall"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8691358024691358"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xrb8bWwCfj4n","outputId":"e91b288a-a01f-40dc-f8b1-10a59b44530c","executionInfo":{"status":"ok","timestamp":1642793880840,"user_tz":-120,"elapsed":16,"user":{"displayName":"Reem Elsayed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13751898496823085405"}}},"source":["precision"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8633540372670807"]},"metadata":{},"execution_count":72}]}]}