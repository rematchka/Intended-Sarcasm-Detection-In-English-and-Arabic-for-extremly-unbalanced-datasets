{"cells":[{"cell_type":"markdown","metadata":{"id":"H08esTFOYO99"},"source":["# Main imports and code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnHQoayhBYlm","outputId":"ef5c19a7-acec-4923-a757-10989f6d1c7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jan  5 20:02:26 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    70W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# check which gpu we're using\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYhFR7nSYOjG","outputId":"a3af739a-882a-4069-bbfe-7d7a5477db49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.7)\n","Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n"]}],"source":["!pip  install transformers\n","!pip install pytorch-ignite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJC8wj73Zd_p"},"outputs":[],"source":["# Any results you write to the current directory are saved as output.\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","from transformers import BertTokenizer,BertModel\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader,Dataset\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","from argparse import ArgumentParser\n","from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n","from ignite.metrics import Accuracy, Loss\n","from ignite.engine.engine import Engine, State, Events\n","from ignite.handlers import EarlyStopping\n","from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n","from ignite.utils import convert_tensor\n","from torch.optim.lr_scheduler import ExponentialLR\n","import warnings  \n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvKeryw3eQPw"},"outputs":[],"source":["import os\n","import gc\n","import copy\n","import time\n","import random\n","import string\n","\n","# For data manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# Pytorch Imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Utils\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","# Sklearn Imports\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, KFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fezs8xASSS28"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel, AdamW\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3FDM6BjTLwW","outputId":"06734108-7363-47d3-9f28-68b24eaef7be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}],"source":["!pip install sentencepiece\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSzWEX54ScQi"},"outputs":[],"source":["import random\n","import os\n","from urllib import request"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i2J-Qx3ekn_N","outputId":"57ffc204-270e-42ab-9579-7dd26b9d39c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["df=pd.read_csv('/content/drive/MyDrive/ISarcasm/DataSet/train.En.csv')\n","df=df[['tweet','sarcastic']]"],"metadata":{"id":"VjoRy3-22MLm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, validate, test = \\\n","              np.split(df.sample(frac=1, random_state=42), \n","                       [int(.6*len(df)), int(.8*len(df))])"],"metadata":{"id":"y6Fd8UBBdTQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train=pd.concat([train, validate], ignore_index=True)"],"metadata":{"id":"PJrh2l2qdXf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tedf1.to_csv('/content/drive/MyDrive/PCL/test_task_1',index=False)"],"metadata":{"id":"X5DMNjrTGT8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trdf1.to_csv('/content/drive/MyDrive/PCL/train_task_1',index=False)"],"metadata":{"id":"l18cECm4GhQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xK6FY70KZ6TY"},"source":["# RoBERTa Baseline for Task 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EA9QzHTCl5F6"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n","\n","from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n","from transformers.data.processors import SingleSentenceClassificationProcessor\n","from transformers import Trainer , TrainingArguments\n","from transformers.trainer_utils import EvaluationStrategy\n","from transformers.data.processors.utils import InputFeatures\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YlT2IDHumPNi","outputId":"388e5a19-7130-4b42-8d77-f3f8b7edc51b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.17.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.9)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}],"source":["!pip install datasets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-E_jNQbV_NL"},"outputs":[],"source":["class PCLTrainDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length,displacemnt):\n","        self.df = df\n","        self.max_len = max_length\n","        self.tokenizer = tokenizer\n","        self.text = df['tweet'].values\n","        self.label=df['sarcastic'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        text = self.text[index]\n","        # summary = self.summary[index]\n","        inputs_text = self.tokenizer.encode_plus(\n","                                text,\n","                                truncation=True,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length'\n","                            )\n","        \n","                            \n","        target = self.label[index]\n","        \n","        text_ids = inputs_text['input_ids']\n","        text_mask = inputs_text['attention_mask']\n","        \n","       \n","        \n","        \n","        return {\n","            \n","            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n","            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n","            'target': torch.tensor(target, dtype=torch.float)\n","        }\n"]},{"cell_type":"code","source":["import math\n","def sigmoid(x):\n","    return 1/(1+math.exp(-x))"],"metadata":{"id":"WLXkQK5Bawae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class F1_Loss(nn.Module):\n","    '''Calculate F1 score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. epsilon <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n","    '''\n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        # assert y_pred.ndim == 2\n","        # assert y_true.ndim == 1\n","        # print(y_pred.shape)\n","        # print(y_true.shape)\n","        # y_pred[y_pred<0.5]=0\n","        # y_pred[y_pred>=0.5]=0\n","\n","\n","        \n","        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2).to(torch.float32)\n","        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n","        \n","        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n","        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n","        f1=f1.detach()\n","        # print(f1.shape)\n","        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n","        # y_true=y_true.reshape((y_true.shape[0], 1))\n","\n","        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n","        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n","\n","\n","        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n","        # print(y_pred)\n","        # print(y_true_one_hot)\n","        CE =torch.nn.CrossEntropyLoss(weight=( 1 - f1))(y_pred, y_true_one_hot)\n","        # loss = ( 1 - f1)  * CE\n","        return  CE.mean()"],"metadata":{"id":"skGp5I4wM_Zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZa-chAxXf5r"},"outputs":[],"source":["# class PCL_Model_Arch(nn.Module):\n","\n","#     def __init__(self):\n","#         super(PCL_Model_Arch, self).__init__()\n","#         self.bert = AutoModel.from_pretrained('roberta-base', output_hidden_states=True)\n","#         output_channel = 16  # number of kernels\n","#         num_classes = 2  # number of targets to predict\n","#         dropout = 0.2  # dropout value\n","#         embedding_dim = 768   # length of embedding dim\n","\n","#         ks = 3  # three conv nets here\n","\n","#         # input_channel = word embeddings at a value of 1; 3 for RGB images\n","#         input_channel = 4  # for single embedding, input_channel = 1\n","\n","#         # [3, 4, 5] = window height\n","#         # padding = padding to account for height of search window\n","\n","#         # 3 convolutional nets\n","#         self.conv1 = nn.Conv2d(input_channel, output_channel, (3, embedding_dim), padding=(2, 0), groups=4)\n","#         self.conv2 = nn.Conv2d(input_channel, output_channel, (4, embedding_dim), padding=(3, 0), groups=4)\n","#         self.conv3 = nn.Conv2d(input_channel, output_channel, (5, embedding_dim), padding=(4, 0), groups=4)\n","\n","#         # apply dropout\n","#         self.dropout = nn.Dropout(dropout)\n","\n","#         # fully connected layer for classification\n","#         # 3x conv nets * output channel\n","#         self.fc1 = nn.Linear(ks * output_channel, num_classes)\n","#         self.softmax = nn.Softmax()\n","\n","#     def forward(self, text_id, text_mask):\n","#         # get the last 4 layers\n","#         outputs= self.bert(text_id, attention_mask=text_mask)\n","#         # all_layers  = [4, 16, 256, 768]\n","#         hidden_layers = outputs[2]  # get hidden layers\n","\n","#         hidden_layers = torch.stack(hidden_layers, dim=1)\n","#         x = hidden_layers[:, -4:] \n","#         # x = x.unsqueeze(1)\n","#         # x = torch.mean(x, 0)\n","#         # print(hidden_layers.size())\n","      \n","#         torch.cuda.empty_cache()\n","#         x = [F.relu(self.conv1(x)).squeeze(3), F.relu(self.conv2(x)).squeeze(3), F.relu(self.conv3(x)).squeeze(3)]\n","#         # max-over-time pooling; # (batch, channel_output) * ks\n","#         x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n","#         # concat results; (batch, channel_output * ks)\n","#         x = torch.cat(x, 1)\n","#         # add dropout\n","#         x = self.dropout(x)\n","#         # generate logits (batch, target_size)\n","#         logit = self.fc1(x)\n","#         torch.cuda.empty_cache()\n","#         return self.softmax(logit)"]},{"cell_type":"code","source":["class PCL_Model_Arch(nn.Module):\n","\n","    def __init__(self):\n","        super(PCL_Model_Arch, self).__init__()\n","        self.bert = AutoModel.from_pretrained('google/canine-c', output_hidden_states=False)\n","        self.drop = nn.Dropout(p=0.2)\n","        self.fc = nn.Linear(768, 2)\n","\n","        \n","        self.softmax = nn.Softmax()\n","\n","    def forward(self, text_id, text_mask):\n","        # get the last 4 layers\n","        outputs= self.bert(text_id, attention_mask=text_mask)\n","        # all_layers  = [4, 16, 256, 768]\n","        hidden_layers = outputs[1]  # get hidden layers\n","        torch.cuda.empty_cache()\n","\n","        \n","        x = self.drop(hidden_layers)\n","        torch.cuda.empty_cache()\n","        # generate logits (batch, target_size)\n","        logit = self.fc(x)\n","        torch.cuda.empty_cache()\n","        return self.softmax(logit)"],"metadata":{"id":"IfQwNPiRx_6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuODW43NYhTz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e16b70a8-9970-4039-ee30-47f969d4523e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n"]}],"source":["tokenizer= AutoTokenizer.from_pretrained('google/canine-c')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0XUHerFY2jd"},"outputs":[],"source":["def criterion(outputs1,  targets):\n","\n","    criterion = F1_Loss()\n","    loss = criterion(outputs1, targets)\n","    return loss"]},{"cell_type":"code","source":["class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n","    \"\"\"\n","    Samples elements randomly from a given list of indices for imbalanced dataset\n","    Arguments:\n","        indices (list, optional): a list of indices\n","        num_samples (int, optional): number of samples to draw\n","    \"\"\"\n","\n","    def __init__(self, dataset, indices=None, num_samples=None):\n","        # if indices is not provided,\n","        # all elements in the dataset will be considered\n","        self.indices = list(range(len(dataset.sarcastic))) \\\n","            if indices is None else indices\n","\n","        # if num_samples is not provided,\n","        # draw `len(indices)` samples in each iteration\n","        self.num_samples = len(self.indices) \\\n","            if num_samples is None else num_samples\n","\n","        # distribution of classes in the dataset\n","        label_to_count = {}\n","        for idx in self.indices:\n","            label = self._get_label(dataset, idx)\n","            if label in label_to_count:\n","                label_to_count[label] += 1\n","            else:\n","                label_to_count[label] = 1\n","\n","        # weight for each sample\n","        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n","        self.weights = torch.DoubleTensor(weights)\n","\n","    def _get_label(self, dataset, id_):\n","        return dataset.sarcastic[id_]\n","\n","    def __iter__(self):\n","        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n","\n","    def __len__(self):\n","        return self.num_samples"],"metadata":{"id":"Zeh7f_UdC6dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRX0b55VaxmS"},"outputs":[],"source":["CONFIG = {\"seed\": 2021,\n","          \"epochs\": 3,\n","          \"model_name\": \"xlnet-base-cased\",\n","          \"train_batch_size\": 16,\n","          \"valid_batch_size\": 64,\n","          \"max_length\": 512,\n","          \"learning_rate\": 1e-4,\n","          \"scheduler\": 'CosineAnnealingLR',\n","          \"min_lr\": 1e-6,\n","          \"T_max\": 500,\n","          \"weight_decay\": 1e-6,\n","          \"n_fold\": 5,\n","          \"n_accumulate\": 1,\n","          \"num_classes\": 1,\n","          \"margin\": 0.5,\n","          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4afl1P8LaD07"},"outputs":[],"source":["def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n","    model.train()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:\n","        \n","        text_ids = data['text_ids'].to(device, dtype = torch.long)\n","        text_mask = data['text_mask'].to(device, dtype = torch.long)\n","        targets = data['target'].to(device, dtype=torch.long)\n","        \n","        batch_size = text_ids.size(0)\n","        # print(targets)\n","\n","        outputs = model(text_ids, text_mask)\n","        # print(outputs.shape)\n","        \n","        # print(outputs.shape)\n","\n","        \n","        loss = criterion(outputs, targets)\n","        loss = loss / CONFIG['n_accumulate']\n","        loss.backward()\n","    \n","        if (step + 1) % CONFIG['n_accumulate'] == 0:\n","            optimizer.step()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            if scheduler is not None:\n","                scheduler.step()\n","                \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n","                        LR=optimizer.param_groups[0]['lr'])\n","    gc.collect()\n","    \n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JdWI_KWRafLZ"},"outputs":[],"source":["@torch.no_grad()\n","def valid_one_epoch(model, dataloader, device, epoch):\n","    model.eval()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:        \n","        \n","        text_ids = data['text_ids'].to(device, dtype = torch.long)\n","        text_mask = data['text_mask'].to(device, dtype = torch.long)\n","        targets = data['target'].to(device, dtype=torch.long)\n","        \n","        batch_size = text_ids.size(0)\n","\n","        outputs = model(text_ids, text_mask)\n","        # outputs = outputs.argmax(dim=1)\n","        \n","        loss = criterion(outputs, targets)\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n","                        LR=optimizer.param_groups[0]['lr'])   \n","    \n","    gc.collect()\n","    \n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1wtBAJJbEou"},"outputs":[],"source":["def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n","    # To automatically log gradients\n","    \n","    if torch.cuda.is_available():\n","        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n","    \n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch_loss = np.inf\n","    history = defaultdict(list)\n","    \n","    for epoch in range(1, num_epochs + 1): \n","        gc.collect()\n","        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n","                                           dataloader=train_loader, \n","                                           device=CONFIG['device'], epoch=epoch)\n","        \n","        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n","                                         epoch=epoch)\n","    \n","        history['Train Loss'].append(train_epoch_loss)\n","        history['Valid Loss'].append(val_epoch_loss)\n","        \n","       \n","        \n","        # deep copy the model\n","        if val_epoch_loss <= best_epoch_loss:\n","            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n","            best_epoch_loss = val_epoch_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            PATH = f\"/content/drive/MyDrive/ISarcasm/Models/canine_c/Loss-Fold-{fold}.bin\"\n","            torch.save(model.state_dict(), PATH)\n","            # Save a model file from the current directory\n","            print(\"Model Saved\")\n","            \n","        print()\n","    \n","    end = time.time()\n","    time_elapsed = end - start\n","    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n","    \n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    \n","    return model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSXiJa2-bRiX"},"outputs":[],"source":["def fetch_scheduler(optimizer):\n","    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n","                                                   eta_min=CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n","                                                             eta_min=CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == None:\n","        return None\n","        \n","    return scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6nXhK_ObVNe"},"outputs":[],"source":["def prepare_loaders(fold):\n","    displacemnt_list=[0,512,1024,1536,2048,2560,3072,3584,4096,4608,4950]\n","    df_train = train[train.kfold != fold].reset_index(drop=True)\n","    df_valid = train[train.kfold == fold].reset_index(drop=True)\n","    sampler = ImbalancedDatasetSampler(df_train)\n","    \n","    train_dataset = PCLTrainDataset(df_train, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=displacemnt_list[fold])\n","    valid_dataset = PCLTrainDataset(df_valid, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=displacemnt_list[fold])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n","                              num_workers=2, shuffle=False, pin_memory=True, drop_last=True,sampler=sampler)\n","    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n","                              num_workers=2, shuffle=False, pin_memory=True)\n","    \n","    return train_loader, valid_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCN4vwBOeaLb"},"outputs":[],"source":["skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n","\n","for fold, ( _, val_) in enumerate(skf.split(X=train, y=train.sarcastic)):\n","    train.loc[val_ , \"kfold\"] = int(fold)\n","    \n","train[\"kfold\"] = train[\"kfold\"].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOtw3nW-2X58"},"outputs":[],"source":["# del model,train_loader, valid_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HoQghNKN2Cy2","outputId":"d37c35a4-cd2b-4660-9174-f24a3645c988"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["130"]},"metadata":{},"execution_count":32}],"source":["import gc\n","gc.collect()"]},{"cell_type":"markdown","source":["http://seekinginference.com/applied_nlp/bert-cnn.html"],"metadata":{"id":"QCwmBcvPlNaO"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXC4SsiDbbGR","outputId":"3b5be655-d87a-40ac-cfcc-ac8fd2e3508e"},"outputs":[{"output_type":"stream","name":"stdout","text":["====== Fold: 0 ======\n","[INFO] Using GPU: Tesla K80\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.342]\n","100%|██████████| 9/9 [00:17<00:00,  1.95s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.328]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (inf ---> 0.32847545066395323)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.337]\n","100%|██████████| 9/9 [00:17<00:00,  1.95s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.487]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.243]\n","100%|██████████| 9/9 [00:17<00:00,  1.95s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.287]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (0.32847545066395323 ---> 0.2873950242459237)\n","Model Saved\n","\n","Training complete in 0h 8m 43s\n","Best Loss: 0.2874\n","\n","====== Fold: 1 ======\n","[INFO] Using GPU: Tesla K80\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.342]\n","100%|██████████| 9/9 [00:17<00:00,  1.95s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.335]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (inf ---> 0.3346926929714443)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.342]\n","100%|██████████| 9/9 [00:17<00:00,  1.96s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.323]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (0.3346926929714443 ---> 0.3226088520105895)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:35<00:00,  1.13s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.341]\n","100%|██████████| 9/9 [00:17<00:00,  1.95s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.329]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training complete in 0h 8m 45s\n","Best Loss: 0.3226\n","\n","====== Fold: 2 ======\n","[INFO] Using GPU: Tesla K80\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.343]\n","100%|██████████| 9/9 [00:17<00:00,  1.94s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.324]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (inf ---> 0.3236971757970415)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:35<00:00,  1.13s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.341]\n","100%|██████████| 9/9 [00:17<00:00,  1.94s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.326]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.342]\n","100%|██████████| 9/9 [00:17<00:00,  1.95s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.334]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training complete in 0h 8m 41s\n","Best Loss: 0.3237\n","\n","====== Fold: 3 ======\n","[INFO] Using GPU: Tesla K80\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.343]\n","100%|██████████| 9/9 [00:17<00:00,  1.92s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.319]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (inf ---> 0.3194562109741005)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.342]\n","100%|██████████| 9/9 [00:17<00:00,  1.94s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.32]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.341]\n","100%|██████████| 9/9 [00:17<00:00,  1.94s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.324]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training complete in 0h 8m 41s\n","Best Loss: 0.3195\n","\n","====== Fold: 4 ======\n","[INFO] Using GPU: Tesla K80\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.341]\n","100%|██████████| 9/9 [00:17<00:00,  1.95s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.302]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Improved (inf ---> 0.30207621610121604)\n","Model Saved\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.342]\n","100%|██████████| 9/9 [00:17<00:00,  1.92s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.334]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [02:34<00:00,  1.12s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.343]\n","100%|██████████| 9/9 [00:17<00:00,  1.93s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.326]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training complete in 0h 8m 40s\n","Best Loss: 0.3021\n","\n"]}],"source":["for fold in range(0, CONFIG['n_fold']):\n","    print(f\"====== Fold: {fold} ======\")\n","\n","    \n","    # Create Dataloaders\n","    train_loader, valid_loader = prepare_loaders(fold=fold)\n","    \n","    model = PCL_Model_Arch()\n","    model.to(CONFIG['device'])\n","    torch.cuda.empty_cache()\n","    \n","    # Define Optimizer and Scheduler\n","    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n","    scheduler = fetch_scheduler(optimizer)\n","    \n","    model, history = run_training(model, optimizer, scheduler,\n","                                  device=CONFIG['device'],\n","                                  num_epochs=CONFIG['epochs'],\n","                                  fold=fold)\n","    \n","    \n","    del model, history, train_loader, valid_loader\n","    _ = gc.collect()\n","    print()"]},{"cell_type":"code","source":["test.dropna(inplace=True)"],"metadata":{"id":"XSvm2lLPAf0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_dataset = PCLTrainDataset(test, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=0)\n","valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n","                              num_workers=2, shuffle=False, pin_memory=True)"],"metadata":{"id":"ujNNMM9HAGtl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def valid_fn(model, dataloader, device):\n","    model.eval()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    PREDS = []\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:\n","        ids = data['text_ids'].to(device, dtype = torch.long)\n","        mask = data['text_mask'].to(device, dtype = torch.long)\n","        \n","        outputs = model(ids, mask)\n","        # outputs = outputs.argmax(dim=1)\n","#         print(len(outputs))\n","#         print(len(np.max(outputs.cpu().detach().numpy(),axis=1)))\n","        PREDS.append(outputs.detach().cpu().numpy()) \n","    \n","    PREDS = np.concatenate(PREDS)\n","    gc.collect()\n","    \n","    return PREDS"],"metadata":{"id":"piuXPwW433gP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference(model_paths, dataloader, device):\n","    final_preds = []\n","    for i, path in enumerate(model_paths):\n","        model = PCL_Model_Arch()\n","        model.to(CONFIG['device'])\n","        model.load_state_dict(torch.load(path))\n","        \n","        print(f\"Getting predictions for model {i+1}\")\n","        preds = valid_fn(model, dataloader, device)\n","        final_preds.append(preds)\n","    \n","    final_preds = np.array(final_preds)\n","    final_preds = np.mean(final_preds, axis=0)\n","    final_preds= np.argmax(final_preds,axis=1)\n","    return final_preds"],"metadata":{"id":"zaxyuvUP3Md0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models/canine_c/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models/canine_c/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models/canine_c/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models/canine_c/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models/canine_c/Loss-Fold-4.bin']\n","preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"],"metadata":{"id":"-mbRltZW4Dvf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a430b12-22f3-4ffc-a4f2-7aaea2aaf614"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Getting predictions for model 1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:21<00:00,  1.91s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Getting predictions for model 2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:20<00:00,  1.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Getting predictions for model 3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:20<00:00,  1.90s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Getting predictions for model 4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:20<00:00,  1.90s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Getting predictions for model 5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [00:20<00:00,  1.89s/it]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKQV51HyommQ"},"outputs":[],"source":["from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n","def print_statistics(y, y_pred):\n","    accuracy = accuracy_score(y, y_pred)\n","    precision =precision_score(y, y_pred, average='weighted')\n","    recall = recall_score(y, y_pred, average='weighted')\n","    f_score = f1_score(y, y_pred, average='weighted')\n","    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n","          % (accuracy, precision, recall, f_score))\n","    print(classification_report(y, y_pred))\n","    return accuracy, precision, recall, f_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3965a5XAooJx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e0369b4-acca-43f9-8ddf-eb51de79ca2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.659\n","Precision: 0.681\n","Recall: 0.659\n","F_score: 0.669\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.75      0.77       531\n","           1       0.31      0.36      0.33       162\n","\n","    accuracy                           0.66       693\n","   macro avg       0.55      0.56      0.55       693\n","weighted avg       0.68      0.66      0.67       693\n","\n","(0.6594516594516594, 0.6805390841693237, 0.6594516594516594, 0.6689318433504481)\n"]}],"source":["print(print_statistics(test['sarcastic'],preds))"]},{"cell_type":"markdown","metadata":{"id":"RT8hjnxbbfJq"},"source":["## Prepare submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7HICl8MJQf0"},"outputs":[],"source":["!cat task1.txt | head -n 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCjziGtxJRif"},"outputs":[],"source":["!cat task2.txt | head -n 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZDLUcYZbhYg"},"outputs":[],"source":["!zip submission.zip task1.txt task2.txt"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CANNIE_c+_f1_CE_loss+_Imbalance_datasampler.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}