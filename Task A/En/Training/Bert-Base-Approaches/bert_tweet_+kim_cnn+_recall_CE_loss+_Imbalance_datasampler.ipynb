{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08esTFOYO99"
      },
      "source": [
        "# Main imports and code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnHQoayhBYlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1cdc08-a81b-4b97-853d-725abc1cee43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan  9 18:06:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check which gpu we're using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYhFR7nSYOjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa776b6-6910-4c87-9681-1955f5ec5b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 513 kB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n",
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[K     |████████████████████████████████| 240 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.7\n"
          ]
        }
      ],
      "source": [
        "!pip  install transformers\n",
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJC8wj73Zd_p"
      },
      "outputs": [],
      "source": [
        "# Any results you write to the current directory are saved as output.\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "from transformers import BertTokenizer,BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from argparse import ArgumentParser\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from ignite.engine.engine import Engine, State, Events\n",
        "from ignite.handlers import EarlyStopping\n",
        "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
        "from ignite.utils import convert_tensor\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import warnings  \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvKeryw3eQPw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "\n",
        "# For data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Pytorch Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Utils\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sklearn Imports\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fezs8xASSS28"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AdamW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3FDM6BjTLwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9717a4f4-2439-438f-cc6b-b8b2e003a863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSzWEX54ScQi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2J-Qx3ekn_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c246e1da-f3a5-4d39-dbd7-f487f9880985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/ISarcasm/DataSet/train.En.csv')\n",
        "df=df[['tweet','sarcastic']]"
      ],
      "metadata": {
        "id": "VjoRy3-22MLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, validate, test = \\\n",
        "              np.split(df.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(df)), int(.8*len(df))])"
      ],
      "metadata": {
        "id": "y6Fd8UBBdTQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.concat([train, validate], ignore_index=True)"
      ],
      "metadata": {
        "id": "PJrh2l2qdXf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tedf1.to_csv('/content/drive/MyDrive/PCL/test_task_1',index=False)"
      ],
      "metadata": {
        "id": "X5DMNjrTGT8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trdf1.to_csv('/content/drive/MyDrive/PCL/train_task_1',index=False)"
      ],
      "metadata": {
        "id": "l18cECm4GhQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6FY70KZ6TY"
      },
      "source": [
        "# RoBERTa Baseline for Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA9QzHTCl5F6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlT2IDHumPNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429e83cc-2ca8-404c-c3cf-80baf821d905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 29.7 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 50.2 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.9)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.17.0 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-E_jNQbV_NL"
      },
      "outputs": [],
      "source": [
        "class PCLTrainDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length,displacemnt):\n",
        "        self.df = df\n",
        "        self.max_len = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = df['tweet'].values\n",
        "        self.label=df['sarcastic'].values\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        # summary = self.summary[index]\n",
        "        inputs_text = self.tokenizer.encode_plus(\n",
        "                                text,\n",
        "                                truncation=True,\n",
        "                                add_special_tokens=True,\n",
        "                                max_length=self.max_len,\n",
        "                                padding='max_length'\n",
        "                            )\n",
        "        \n",
        "                            \n",
        "        target = self.label[index]\n",
        "        \n",
        "        text_ids = inputs_text['input_ids']\n",
        "        text_mask = inputs_text['attention_mask']\n",
        "        \n",
        "       \n",
        "        \n",
        "        \n",
        "        return {\n",
        "            \n",
        "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
        "            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
        "            'target': torch.tensor(target, dtype=torch.float)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))"
      ],
      "metadata": {
        "id": "WLXkQK5Bawae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Recall_Loss(nn.Module):\n",
        "    '''Calculate F1 score. Can work with gpu tensors\n",
        "    \n",
        "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        `ndim` == 1. epsilon <= val <= 1\n",
        "    \n",
        "    Reference\n",
        "    ---------\n",
        "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
        "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
        "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
        "    '''\n",
        "    def __init__(self, epsilon=1e-7):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "    def forward(self, y_pred, y_true,):\n",
        "        # assert y_pred.ndim == 2\n",
        "        # assert y_true.ndim == 1\n",
        "        # print(y_pred.shape)\n",
        "        # print(y_true.shape)\n",
        "        # y_pred[y_pred<0.5]=0\n",
        "        # y_pred[y_pred>=0.5]=0\n",
        "\n",
        "\n",
        "        \n",
        "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2).to(torch.float32)\n",
        "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
        "        \n",
        "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
        "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
        "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        # f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
        "        # f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
        "        # f1=f1.detach()\n",
        "        # print(f1.shape)\n",
        "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
        "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
        "\n",
        "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
        "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
        "\n",
        "\n",
        "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
        "        # print(y_pred)\n",
        "        # print(y_true_one_hot)\n",
        "        recall=recall.detach()\n",
        "        CE =torch.nn.CrossEntropyLoss(weight=( 1 - recall))(y_pred, y_true_one_hot)\n",
        "        # loss = ( 1 - f1)  * CE\n",
        "        return  CE.mean()"
      ],
      "metadata": {
        "id": "skGp5I4wM_Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZa-chAxXf5r"
      },
      "outputs": [],
      "source": [
        "class PCL_Model_Arch(nn.Module):\n",
        "    def __init__(self,pre_trained='vinai/bertweet-base'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = AutoModel.from_pretrained(pre_trained, output_hidden_states=True)\n",
        "        output_channel = 16  # number of kernels\n",
        "        num_classes = 2  # number of targets to predict\n",
        "        dropout = 0.2  # dropout value\n",
        "        embedding_dim = 768   # length of embedding dim\n",
        "\n",
        "        ks = 3  # three conv nets here\n",
        "\n",
        "        # input_channel = word embeddings at a value of 1; 3 for RGB images\n",
        "        input_channel = 4  # for single embedding, input_channel = 1\n",
        "\n",
        "        # [3, 4, 5] = window height\n",
        "        # padding = padding to account for height of search window\n",
        "\n",
        "        # 3 convolutional nets\n",
        "        self.conv1 = nn.Conv2d(input_channel, output_channel, (3, embedding_dim), padding=(2, 0), groups=4)\n",
        "        self.conv2 = nn.Conv2d(input_channel, output_channel, (4, embedding_dim), padding=(3, 0), groups=4)\n",
        "        self.conv3 = nn.Conv2d(input_channel, output_channel, (5, embedding_dim), padding=(4, 0), groups=4)\n",
        "\n",
        "        # apply dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # fully connected layer for classification\n",
        "        # 3x conv nets * output channel\n",
        "        self.fc1 = nn.Linear(ks * output_channel, num_classes)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, text_id, text_mask):\n",
        "        # get the last 4 layers\n",
        "        outputs= self.bert(text_id, attention_mask=text_mask)\n",
        "        # all_layers  = [4, 16, 256, 768]\n",
        "        hidden_layers = outputs[2]  # get hidden layers\n",
        "\n",
        "        hidden_layers = torch.stack(hidden_layers, dim=1)\n",
        "        x = hidden_layers[:, -4:] \n",
        "        # x = x.unsqueeze(1)\n",
        "        # x = torch.mean(x, 0)\n",
        "        # print(hidden_layers.size())\n",
        "      \n",
        "        torch.cuda.empty_cache()\n",
        "        x = [F.relu(self.conv1(x)).squeeze(3), F.relu(self.conv2(x)).squeeze(3), F.relu(self.conv3(x)).squeeze(3)]\n",
        "        # max-over-time pooling; # (batch, channel_output) * ks\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        # concat results; (batch, channel_output * ks)\n",
        "        x = torch.cat(x, 1)\n",
        "        # add dropout\n",
        "        x = self.dropout(x)\n",
        "        # generate logits (batch, target_size)\n",
        "        logit = self.fc1(x)\n",
        "        torch.cuda.empty_cache()\n",
        "        return self.softmax(logit)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class PCL_Model_Arch(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(PCL_Model_Arch, self).__init__()\n",
        "#         self.bert = AutoModel.from_pretrained('google/canine-c', output_hidden_states=False)\n",
        "#         self.drop = nn.Dropout(p=0.2)\n",
        "#         self.fc = nn.Linear(768, 2)\n",
        "\n",
        "        \n",
        "#         self.softmax = nn.Softmax()\n",
        "\n",
        "#     def forward(self, text_id, text_mask):\n",
        "#         # get the last 4 layers\n",
        "#         outputs= self.bert(text_id, attention_mask=text_mask)\n",
        "#         # all_layers  = [4, 16, 256, 768]\n",
        "#         hidden_layers = outputs[1]  # get hidden layers\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "        \n",
        "#         x = self.drop(hidden_layers)\n",
        "#         torch.cuda.empty_cache()\n",
        "#         # generate logits (batch, target_size)\n",
        "#         logit = self.fc(x)\n",
        "#         torch.cuda.empty_cache()\n",
        "#         return self.softmax(logit)"
      ],
      "metadata": {
        "id": "IfQwNPiRx_6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT2dbUWN294h",
        "outputId": "55283fbe-32da-4874-ae77-4bb17d74f2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuODW43NYhTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf65cb1-7c92-4604-cd15-ca488de38950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer= AutoTokenizer.from_pretrained('vinai/bertweet-base')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0XUHerFY2jd"
      },
      "outputs": [],
      "source": [
        "def criterion(outputs1,  targets):\n",
        "\n",
        "    criterion = Recall_Loss()\n",
        "    loss = criterion(outputs1, targets)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"\n",
        "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
        "    Arguments:\n",
        "        indices (list, optional): a list of indices\n",
        "        num_samples (int, optional): number of samples to draw\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, indices=None, num_samples=None):\n",
        "        # if indices is not provided,\n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset.sarcastic))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # if num_samples is not provided,\n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "\n",
        "        # distribution of classes in the dataset\n",
        "        label_to_count = {}\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label in label_to_count:\n",
        "                label_to_count[label] += 1\n",
        "            else:\n",
        "                label_to_count[label] = 1\n",
        "\n",
        "        # weight for each sample\n",
        "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, id_):\n",
        "        return dataset.sarcastic[id_]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples"
      ],
      "metadata": {
        "id": "Zeh7f_UdC6dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRX0b55VaxmS"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\"seed\": 2021,\n",
        "          \"epochs\": 3,\n",
        "          \"model_name\": \"xlnet-base-cased\",\n",
        "          \"train_batch_size\": 16,\n",
        "          \"valid_batch_size\": 64,\n",
        "          \"max_length\": 120,\n",
        "          \"learning_rate\": 1e-4,\n",
        "          \"scheduler\": 'CosineAnnealingLR',\n",
        "          \"min_lr\": 1e-6,\n",
        "          \"T_max\": 500,\n",
        "          \"weight_decay\": 1e-6,\n",
        "          \"n_fold\": 5,\n",
        "          \"n_accumulate\": 1,\n",
        "          \"num_classes\": 1,\n",
        "          \"margin\": 0.5,\n",
        "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4afl1P8LaD07"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:\n",
        "        \n",
        "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
        "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
        "        targets = data['target'].to(device, dtype=torch.long)\n",
        "        # print(text_ids.shape)\n",
        "        \n",
        "        batch_size = text_ids.size(0)\n",
        "        # print(targets)\n",
        "\n",
        "        outputs = model(text_ids, text_mask)\n",
        "        # print(outputs.shape)\n",
        "        \n",
        "        # print(outputs.shape)\n",
        "\n",
        "        \n",
        "        loss = criterion(outputs, targets)\n",
        "        loss = loss / CONFIG['n_accumulate']\n",
        "        loss.backward()\n",
        "    \n",
        "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
        "            optimizer.step()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                \n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "        \n",
        "        epoch_loss = running_loss / dataset_size\n",
        "        \n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
        "                        LR=optimizer.param_groups[0]['lr'])\n",
        "    gc.collect()\n",
        "    \n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdWI_KWRafLZ"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "    \n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:        \n",
        "        \n",
        "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
        "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
        "        targets = data['target'].to(device, dtype=torch.long)\n",
        "        # print(text_ids.shape)\n",
        "        \n",
        "        batch_size = text_ids.size(0)\n",
        "\n",
        "        outputs = model(text_ids, text_mask)\n",
        "        # outputs = outputs.argmax(dim=1)\n",
        "        \n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "        \n",
        "        epoch_loss = running_loss / dataset_size\n",
        "        \n",
        "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
        "                        LR=optimizer.param_groups[0]['lr'])   \n",
        "    \n",
        "    gc.collect()\n",
        "    \n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1wtBAJJbEou"
      },
      "outputs": [],
      "source": [
        "def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n",
        "    # To automatically log gradients\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "    \n",
        "    start = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_epoch_loss = np.inf\n",
        "    history = defaultdict(list)\n",
        "    \n",
        "    for epoch in range(1, num_epochs + 1): \n",
        "        gc.collect()\n",
        "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
        "                                           dataloader=train_loader, \n",
        "                                           device=CONFIG['device'], epoch=epoch)\n",
        "        \n",
        "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
        "                                         epoch=epoch)\n",
        "    \n",
        "        history['Train Loss'].append(train_epoch_loss)\n",
        "        history['Valid Loss'].append(val_epoch_loss)\n",
        "        \n",
        "       \n",
        "        \n",
        "        # deep copy the model\n",
        "        if val_epoch_loss <= best_epoch_loss:\n",
        "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
        "            best_epoch_loss = val_epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            PATH = f\"/content/drive/MyDrive/ISarcasm/Models/bert_tweet_kim_cnn/Loss-Fold-{fold}.bin\"\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            # Save a model file from the current directory\n",
        "            print(\"Model Saved\")\n",
        "            \n",
        "        print()\n",
        "    \n",
        "    end = time.time()\n",
        "    time_elapsed = end - start\n",
        "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
        "    \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSXiJa2-bRiX"
      },
      "outputs": [],
      "source": [
        "def fetch_scheduler(optimizer):\n",
        "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
        "                                                   eta_min=CONFIG['min_lr'])\n",
        "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
        "                                                             eta_min=CONFIG['min_lr'])\n",
        "    elif CONFIG['scheduler'] == None:\n",
        "        return None\n",
        "        \n",
        "    return scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6nXhK_ObVNe"
      },
      "outputs": [],
      "source": [
        "def prepare_loaders(fold):\n",
        "    displacemnt_list=[0,512,1024,1536,2048,2560,3072,3584,4096,4608,4950]\n",
        "    df_train = train[train.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = train[train.kfold == fold].reset_index(drop=True)\n",
        "    sampler = ImbalancedDatasetSampler(df_train)\n",
        "    \n",
        "    train_dataset = PCLTrainDataset(df_train, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=displacemnt_list[fold])\n",
        "    valid_dataset = PCLTrainDataset(df_valid, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=displacemnt_list[fold])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
        "                              num_workers=2, shuffle=False, pin_memory=True, drop_last=True,sampler=sampler)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
        "                              num_workers=2, shuffle=False, pin_memory=True)\n",
        "    \n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCN4vwBOeaLb"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n",
        "\n",
        "for fold, ( _, val_) in enumerate(skf.split(X=train, y=train.sarcastic)):\n",
        "    train.loc[val_ , \"kfold\"] = int(fold)\n",
        "    \n",
        "train[\"kfold\"] = train[\"kfold\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOtw3nW-2X58"
      },
      "outputs": [],
      "source": [
        "# del model,train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoQghNKN2Cy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e0f7fb-6d6c-4988-cf0c-04ec40ae6363"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "254"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://seekinginference.com/applied_nlp/bert-cnn.html"
      ],
      "metadata": {
        "id": "QCwmBcvPlNaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXC4SsiDbbGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157abeba-5488-4797-d676-125bfc5252b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Fold: 1 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.327]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.13s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.309]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 0.30892189778723156)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.197]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.13s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (0.30892189778723156 ---> 0.2202721588246457)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.0912]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.13s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (0.2202721588246457 ---> 0.2099062472850353)\n",
            "Model Saved\n",
            "\n",
            "Training complete in 0h 21m 41s\n",
            "Best Loss: 0.2099\n",
            "\n",
            "====== Fold: 2 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.285]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.13s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.337]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 0.33737168376510207)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.163]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.208]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (0.33737168376510207 ---> 0.20809215135402506)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:55<00:00,  3.01s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.0863]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.169]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (0.20809215135402506 ---> 0.16945130419623744)\n",
            "Model Saved\n",
            "\n",
            "Training complete in 0h 21m 30s\n",
            "Best Loss: 0.1695\n",
            "\n",
            "====== Fold: 3 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.346]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.374]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 0.373920184642345)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.347]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.11s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.344]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (0.373920184642345 ---> 0.34364201915156734)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.02s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.347]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.11s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 21m 27s\n",
            "Best Loss: 0.3436\n",
            "\n",
            "====== Fold: 4 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:55<00:00,  3.01s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.284]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.228]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 0.2282688189499645)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:55<00:00,  3.01s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.145]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.13s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.202]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (0.2282688189499645 ---> 0.20241339380990728)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:55<00:00,  3.01s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.0838]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.11s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 21m 27s\n",
            "Best Loss: 0.2024\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for fold in range(1, CONFIG['n_fold']):\n",
        "    print(f\"====== Fold: {fold} ======\")\n",
        "\n",
        "    \n",
        "    # Create Dataloaders\n",
        "    train_loader, valid_loader = prepare_loaders(fold=fold)\n",
        "    \n",
        "    model = PCL_Model_Arch()\n",
        "    model.to(CONFIG['device'])\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Define Optimizer and Scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "    scheduler = fetch_scheduler(optimizer)\n",
        "    \n",
        "    model, history = run_training(model, optimizer, scheduler,\n",
        "                                  device=CONFIG['device'],\n",
        "                                  num_epochs=CONFIG['epochs'],\n",
        "                                  fold=fold)\n",
        "    \n",
        "    \n",
        "    del model, history, train_loader, valid_loader\n",
        "    _ = gc.collect()\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(0, 1):\n",
        "    print(f\"====== Fold: {fold} ======\")\n",
        "\n",
        "    \n",
        "    # Create Dataloaders\n",
        "    train_loader, valid_loader = prepare_loaders(fold=fold)\n",
        "    \n",
        "    model = PCL_Model_Arch()\n",
        "    model.to(CONFIG['device'])\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Define Optimizer and Scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "    scheduler = fetch_scheduler(optimizer)\n",
        "    \n",
        "    model, history = run_training(model, optimizer, scheduler,\n",
        "                                  device=CONFIG['device'],\n",
        "                                  num_epochs=CONFIG['epochs'],\n",
        "                                  fold=fold)\n",
        "    \n",
        "    \n",
        "    del model, history, train_loader, valid_loader\n",
        "    _ = gc.collect()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iS3tXLk-SCc",
        "outputId": "c95cf266-6550-4af6-e470-3b62d95f00ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Fold: 0 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:56<00:00,  3.01s/it, Epoch=1, LR=8.25e-5, Train_Loss=0.281]\n",
            "100%|██████████| 9/9 [00:10<00:00,  1.12s/it, Epoch=1, LR=8.25e-5, Valid_Loss=0.212]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 0.21237278027577444)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:55<00:00,  3.01s/it, Epoch=2, LR=4.24e-5, Train_Loss=0.333]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.11s/it, Epoch=2, LR=4.24e-5, Valid_Loss=0.336]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 138/138 [06:55<00:00,  3.01s/it, Epoch=3, LR=8.05e-6, Train_Loss=0.341]\n",
            "100%|██████████| 9/9 [00:09<00:00,  1.11s/it, Epoch=3, LR=8.05e-6, Valid_Loss=0.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 21m 23s\n",
            "Best Loss: 0.2124\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "XSvm2lLPAf0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = PCLTrainDataset(test, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=0)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
        "                              num_workers=2, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "ujNNMM9HAGtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def valid_fn(model, dataloader, device):\n",
        "    model.eval()\n",
        "    \n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    PREDS = []\n",
        "    \n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:\n",
        "        ids = data['text_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['text_mask'].to(device, dtype = torch.long)\n",
        "        \n",
        "        outputs = model(ids, mask)\n",
        "        # outputs = outputs.argmax(dim=1)\n",
        "#         print(len(outputs))\n",
        "#         print(len(np.max(outputs.cpu().detach().numpy(),axis=1)))\n",
        "        PREDS.append(outputs.detach().cpu().numpy()) \n",
        "    \n",
        "    PREDS = np.concatenate(PREDS)\n",
        "    gc.collect()\n",
        "    \n",
        "    return PREDS"
      ],
      "metadata": {
        "id": "piuXPwW433gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model_paths, dataloader, device):\n",
        "    final_preds = []\n",
        "    for i, path in enumerate(model_paths):\n",
        "        model = PCL_Model_Arch()\n",
        "        model.to(CONFIG['device'])\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        \n",
        "        print(f\"Getting predictions for model {i+1}\")\n",
        "        preds = valid_fn(model, dataloader, device)\n",
        "        final_preds.append(preds)\n",
        "    \n",
        "    final_preds = np.array(final_preds)\n",
        "    final_preds = np.mean(final_preds, axis=0)\n",
        "    final_preds= np.argmax(final_preds,axis=1)\n",
        "    return final_preds"
      ],
      "metadata": {
        "id": "zaxyuvUP3Md0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models/bert_tweet_kim_cnn/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models/bert_tweet_kim_cnn/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models/bert_tweet_kim_cnn/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models/bert_tweet_kim_cnn/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models/bert_tweet_kim_cnn/Loss-Fold-4.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ],
      "metadata": {
        "id": "-mbRltZW4Dvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59bcbd96-0e32-45b4-c659-f6d14c7d0a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:12<00:00,  1.12s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:12<00:00,  1.12s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:12<00:00,  1.12s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:12<00:00,  1.13s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:12<00:00,  1.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKQV51HyommQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n",
        "def print_statistics(y, y_pred):\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision =precision_score(y, y_pred, average='weighted')\n",
        "    recall = recall_score(y, y_pred, average='weighted')\n",
        "    f_score = f1_score(y, y_pred, average='weighted')\n",
        "    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n",
        "          % (accuracy, precision, recall, f_score))\n",
        "    print(classification_report(y, y_pred))\n",
        "    return accuracy, precision, recall, f_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3965a5XAooJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b945684-49f4-44da-cd47-fe1cb595531f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.788\n",
            "Precision: 0.795\n",
            "Recall: 0.788\n",
            "F_score: 0.791\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86       531\n",
            "           1       0.54      0.59      0.57       162\n",
            "\n",
            "    accuracy                           0.79       693\n",
            "   macro avg       0.71      0.72      0.71       693\n",
            "weighted avg       0.80      0.79      0.79       693\n",
            "\n",
            "(0.7878787878787878, 0.7950155874870103, 0.7878787878787878, 0.7910522421261067)\n"
          ]
        }
      ],
      "source": [
        "print(print_statistics(test['sarcastic'],preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT8hjnxbbfJq"
      },
      "source": [
        "## Prepare submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7HICl8MJQf0"
      },
      "outputs": [],
      "source": [
        "!cat task1.txt | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCjziGtxJRif"
      },
      "outputs": [],
      "source": [
        "!cat task2.txt | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZDLUcYZbhYg"
      },
      "outputs": [],
      "source": [
        "!zip submission.zip task1.txt task2.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "bert tweet +kim cnn+ recall CE loss+ Imbalance datasampler.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}