{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08esTFOYO99"
      },
      "source": [
        "# Main imports and code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnHQoayhBYlm",
        "outputId": "8172d207-af32-4a0a-d646-5fc8aa0d0353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 27 15:21:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check which gpu we're using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYhFR7nSYOjG",
        "outputId": "0cdb8cdc-58c4-48c2-e947-d220d69f8362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 25.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n",
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.8-py3-none-any.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.8\n"
          ]
        }
      ],
      "source": [
        "!pip  install transformers\n",
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJC8wj73Zd_p"
      },
      "outputs": [],
      "source": [
        "# Any results you write to the current directory are saved as output.\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "from transformers import BertTokenizer,BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from argparse import ArgumentParser\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from ignite.engine.engine import Engine, State, Events\n",
        "from ignite.handlers import EarlyStopping\n",
        "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
        "from ignite.utils import convert_tensor\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import warnings  \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvKeryw3eQPw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "\n",
        "# For data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Pytorch Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Utils\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sklearn Imports\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fezs8xASSS28"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AdamW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3FDM6BjTLwW",
        "outputId": "2a7af16e-9d46-4177-8945-392f4dd4849d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSzWEX54ScQi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2J-Qx3ekn_N",
        "outputId": "6c6256c8-dd35-4193-a18a-c4e751dc72ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjoRy3-22MLm"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/ISarcasm/DataSet/train.En.csv')\n",
        "df=df.loc[df['sarcastic']==1]\n",
        "df=df[['tweet','sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Fd8UBBdTQ1"
      },
      "outputs": [],
      "source": [
        "train, validate, test = \\\n",
        "              np.split(df.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(df)), int(.8*len(df))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJrh2l2qdXf_"
      },
      "outputs": [],
      "source": [
        "train=pd.concat([train, validate], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5DMNjrTGT8T"
      },
      "outputs": [],
      "source": [
        "# tedf1.to_csv('/content/drive/MyDrive/PCL/test_task_1',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l18cECm4GhQv"
      },
      "outputs": [],
      "source": [
        "# trdf1.to_csv('/content/drive/MyDrive/PCL/train_task_1',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6FY70KZ6TY"
      },
      "source": [
        "# RoBERTa Baseline for Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA9QzHTCl5F6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlT2IDHumPNi",
        "outputId": "a966d745-a636-4a31-bca7-6e76311e84a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 311 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 24.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 35.7 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 34.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.1 frozenlist-1.3.0 fsspec-2022.1.0 multidict-6.0.2 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-E_jNQbV_NL"
      },
      "outputs": [],
      "source": [
        "class PCLTrainDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length,displacemnt):\n",
        "        self.df = df\n",
        "        self.max_len = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = df['tweet'].values\n",
        "        self.label=df[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        # summary = self.summary[index]\n",
        "        inputs_text = self.tokenizer.encode_plus(\n",
        "                                text,\n",
        "                                truncation=True,\n",
        "                                add_special_tokens=True,\n",
        "                                max_length=self.max_len,\n",
        "                                padding='max_length'\n",
        "                            )\n",
        "        \n",
        "                            \n",
        "        target = self.label[index]\n",
        "        \n",
        "        text_ids = inputs_text['input_ids']\n",
        "        text_mask = inputs_text['attention_mask']\n",
        "        \n",
        "       \n",
        "        \n",
        "        \n",
        "        return {\n",
        "            \n",
        "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
        "            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
        "            'target': torch.tensor(target, dtype=torch.float)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLXkQK5Bawae"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IDii9DvXR5G"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class F1Score:\n",
        "    \"\"\"\n",
        "    Class for f1 calculation in Pytorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, average: str = 'weighted'):\n",
        "        \"\"\"\n",
        "        Init.\n",
        "\n",
        "        Args:\n",
        "            average: averaging method\n",
        "        \"\"\"\n",
        "        self.average = average\n",
        "        if average not in [None, 'micro', 'macro', 'weighted']:\n",
        "            raise ValueError('Wrong value of average parameter')\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_f1_micro(predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate f1 micro.\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "\n",
        "        Returns:\n",
        "            f1 score\n",
        "        \"\"\"\n",
        "        true_positive = torch.eq(labels, predictions).sum().float()\n",
        "        f1_score = torch.div(true_positive, len(labels))\n",
        "        return f1_score\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_f1_count_for_label(predictions: torch.Tensor,\n",
        "                                labels: torch.Tensor, label_id: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calculate f1 and true count for the label\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "            label_id: id of current label\n",
        "\n",
        "        Returns:\n",
        "            f1 score and true count for label\n",
        "        \"\"\"\n",
        "        # label count\n",
        "        true_count = torch.eq(labels, label_id).sum()\n",
        "\n",
        "        # true positives: labels equal to prediction and to label_id\n",
        "        true_positive = torch.logical_and(torch.eq(labels, predictions),\n",
        "                                          torch.eq(labels, label_id)).sum().float()\n",
        "        # precision for label\n",
        "        precision = torch.div(true_positive, torch.eq(predictions, label_id).sum().float())\n",
        "        # replace nan values with 0\n",
        "        precision = torch.where(torch.isnan(precision),\n",
        "                                torch.zeros_like(precision).type_as(true_positive),\n",
        "                                precision)\n",
        "\n",
        "        # recall for label\n",
        "        recall = torch.div(true_positive, true_count)\n",
        "        # f1\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "        # replace nan values with 0\n",
        "        f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1).type_as(true_positive), f1)\n",
        "        return f1, true_count\n",
        "\n",
        "    def __call__(self, predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate f1 score based on averaging method defined in init.\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "\n",
        "        Returns:\n",
        "            f1 score\n",
        "        \"\"\"\n",
        "\n",
        "        # simpler calculation for micro\n",
        "        if self.average == 'micro':\n",
        "            return self.calc_f1_micro(predictions, labels)\n",
        "\n",
        "        f1_score = 0\n",
        "        for label_id in range(1, len(labels.unique()) + 1):\n",
        "            f1, true_count = self.calc_f1_count_for_label(predictions, labels, label_id)\n",
        "\n",
        "            if self.average == 'weighted':\n",
        "                f1_score += f1 * true_count\n",
        "            elif self.average == 'macro':\n",
        "                f1_score += f1\n",
        "\n",
        "        if self.average == 'weighted':\n",
        "            f1_score = torch.div(f1_score, len(labels))\n",
        "        elif self.average == 'macro':\n",
        "            f1_score = torch.div(f1_score, len(labels.unique()))\n",
        "\n",
        "        return f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skGp5I4wM_Zn"
      },
      "outputs": [],
      "source": [
        "class Recall_Loss(nn.Module):\n",
        "    '''Calculate F1 score. Can work with gpu tensors\n",
        "    \n",
        "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        `ndim` == 1. epsilon <= val <= 1\n",
        "    \n",
        "    Reference\n",
        "    ---------\n",
        "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
        "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
        "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
        "    '''\n",
        "    def __init__(self, epsilon=1e-7):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "    def forward(self, y_pred, y_true,):\n",
        "        # assert y_pred.ndim == 2\n",
        "        # assert y_true.ndim == 1\n",
        "        # print(y_pred.shape)\n",
        "        # print(y_true.shape)\n",
        "        # y_pred[y_pred<0.5]=0\n",
        "        # y_pred[y_pred>=0.5]=0\n",
        "\n",
        "\n",
        "        \n",
        "        y_true_one_hot = y_true.to(torch.float32)\n",
        "        # y_pred_one_hot = F.one_hot(y_pred.to(torch.int64), 2).to(torch.float32)\n",
        "        \n",
        "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
        "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
        "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        # f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
        "        # f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
        "        # f1=f1.detach()\n",
        "        # print(f1.shape)\n",
        "        # y_pred=y_pred.reshape((y_pred.shape[0], 1))\n",
        "        # y_true=y_true.reshape((y_true.shape[0], 1))\n",
        "\n",
        "        # p1=y_true*(math.log(sigmoid(y_pred)))*(1-f1)[1]\n",
        "        # p0=(1-y_true)*math.log(1-sigmoid(y_pred))*(1-f1)[0]\n",
        "\n",
        "\n",
        "        # y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2)\n",
        "        # print(y_pred)\n",
        "        # print(y_true_one_hot)\n",
        "        recall=recall.detach()\n",
        "        # f1= F1Score('macro')(y_pred, y_true_one_hot)\n",
        "        # f1=f1.detach()\n",
        "        CE =torch.nn.BCEWithLogitsLoss(weight=( 1 - recall))(y_pred, y_true_one_hot)\n",
        "        # loss = ( 1 - f1)  * CE\n",
        "        return  CE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZjiLBuJkShx"
      },
      "outputs": [],
      "source": [
        "def _squeeze_binary_labels(label):\n",
        "    if label.size(1) == 1:\n",
        "        squeeze_label = label.view(len(label), -1)\n",
        "    else:\n",
        "        inds = torch.nonzero(label >= 1).squeeze()\n",
        "        squeeze_label = inds[:,-1]\n",
        "    return squeeze_label\n",
        "\n",
        "def cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None):\n",
        "    # element-wise losses\n",
        "    if label.size(-1) != pred.size(0):\n",
        "        label = _squeeze_binary_labels(label)\n",
        "\n",
        "    loss = F.cross_entropy(pred, label, reduction='none')\n",
        "\n",
        "    # apply weights and do the reduction\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "    loss = weight_reduce_loss(\n",
        "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def _expand_binary_labels(labels, label_weights, label_channels):\n",
        "    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n",
        "    inds = torch.nonzero(labels >= 1).squeeze()\n",
        "    if inds.numel() > 0:\n",
        "        bin_labels[inds, labels[inds] - 1] = 1\n",
        "    if label_weights is None:\n",
        "        bin_label_weights = None\n",
        "    else:\n",
        "        bin_label_weights = label_weights.view(-1, 1).expand(\n",
        "            label_weights.size(0), label_channels)\n",
        "    return bin_labels, bin_label_weights\n",
        "\n",
        "\n",
        "def binary_cross_entropy(pred,\n",
        "                         label,\n",
        "                         weight=None,\n",
        "                         reduction='mean',\n",
        "                         avg_factor=None):\n",
        "    if pred.dim() != label.dim():\n",
        "        label, weight = _expand_binary_labels(label, weight, pred.size(-1))\n",
        "\n",
        "    # weighted element-wise losses\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(\n",
        "        pred, label.float(), weight, reduction='none')\n",
        "    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def partial_cross_entropy(pred,\n",
        "                          label,\n",
        "                          weight=None,\n",
        "                          reduction='mean',\n",
        "                          avg_factor=None):\n",
        "    if pred.dim() != label.dim():\n",
        "        label, weight = _expand_binary_labels(label, weight, pred.size(-1))\n",
        "\n",
        "    # weighted element-wise losses\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "\n",
        "    mask = label == -1\n",
        "    loss = F.binary_cross_entropy_with_logits(\n",
        "        pred, label.float(), weight, reduction='none')\n",
        "    if mask.sum() > 0:\n",
        "        loss *= (1-mask).float()\n",
        "        avg_factor = (1-mask).float().sum()\n",
        "\n",
        "    # do the reduction for the weighted loss\n",
        "    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def kpos_cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None):\n",
        "    # element-wise losses\n",
        "    if pred.dim() != label.dim():\n",
        "        label, weight = _expand_binary_labels(label, weight, pred.size(-1))\n",
        "\n",
        "    target = label.float() / torch.sum(label, dim=1, keepdim=True).float()\n",
        "\n",
        "    loss = - target * F.log_softmax(pred, dim=1)\n",
        "    # apply weights and do the reduction\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "    loss = weight_reduce_loss(\n",
        "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss\n",
        "\n",
        "class CrossEntropyLoss(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 use_sigmoid=False,\n",
        "                 use_kpos=False,\n",
        "                 partial=False,\n",
        "                 reduction='mean',\n",
        "                 loss_weight=1.0,\n",
        "                 thrds=None):\n",
        "        super(CrossEntropyLoss, self).__init__()\n",
        "        assert (use_sigmoid is True) or (partial is False)\n",
        "        self.use_sigmoid = use_sigmoid\n",
        "        self.use_kpos = use_kpos\n",
        "        self.partial = partial\n",
        "        self.reduction = reduction\n",
        "        self.loss_weight = loss_weight\n",
        "        if self.use_sigmoid and thrds is not None:\n",
        "            self.thrds=inverse_sigmoid(thrds)\n",
        "        else:\n",
        "            self.thrds = thrds\n",
        "\n",
        "        if self.use_sigmoid:\n",
        "            if self.partial:\n",
        "                self.cls_criterion = partial_cross_entropy\n",
        "            else:\n",
        "                self.cls_criterion = binary_cross_entropy\n",
        "        elif self.use_kpos:\n",
        "            self.cls_criterion = kpos_cross_entropy\n",
        "        else:\n",
        "            self.cls_criterion = cross_entropy\n",
        "\n",
        "    def forward(self,\n",
        "                cls_score,\n",
        "                label,\n",
        "                weight=None,\n",
        "                avg_factor=None,\n",
        "                reduction_override=None,\n",
        "                **kwargs):\n",
        "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
        "        reduction = (\n",
        "            reduction_override if reduction_override else self.reduction)\n",
        "\n",
        "        if self.thrds is not None:\n",
        "            cut_high_mask = (label == 1) * (cls_score > self.thrds[1])\n",
        "            cut_low_mask = (label == 0) * (cls_score < self.thrds[0])\n",
        "            if weight is not None:\n",
        "                weight *= (1 - cut_high_mask).float() * (1 - cut_low_mask).float()\n",
        "            else:\n",
        "                weight = (1 - cut_high_mask).float() * (1 - cut_low_mask).float()\n",
        "\n",
        "        loss_cls = self.loss_weight * self.cls_criterion(\n",
        "            cls_score,\n",
        "            label,\n",
        "            weight,\n",
        "            reduction=reduction,\n",
        "            avg_factor=avg_factor,\n",
        "            **kwargs)\n",
        "        return loss_cls\n",
        "\n",
        "def inverse_sigmoid(Y):\n",
        "    X = []\n",
        "    for y in Y:\n",
        "        y = max(y,1e-14)\n",
        "        if y == 1:\n",
        "            x = 1e10\n",
        "        else:\n",
        "            x = -np.log(1/y-1)\n",
        "        X.append(x)\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7zDNZday-4w"
      },
      "outputs": [],
      "source": [
        "class ResampleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 use_sigmoid=True, partial=False,\n",
        "                 loss_weight=1.0, reduction='mean',\n",
        "                 reweight_func=None,  # None, 'inv', 'sqrt_inv', 'rebalance', 'CB'\n",
        "                 weight_norm=None, # None, 'by_instance', 'by_batch'\n",
        "                 focal=dict(\n",
        "                     focal=True,\n",
        "                     alpha=0.5,\n",
        "                     gamma=2,\n",
        "                 ),\n",
        "                 map_param=dict(\n",
        "                     alpha=10.0,\n",
        "                     beta=0.2,\n",
        "                     gamma=0.1\n",
        "                 ),\n",
        "                 CB_loss=dict(\n",
        "                     CB_beta=0.9,\n",
        "                     CB_mode='average_w'  # 'by_class', 'average_n', 'average_w', 'min_n'\n",
        "                 ),\n",
        "                 logit_reg=dict(\n",
        "                     neg_scale=5.0,\n",
        "                     init_bias=0.1\n",
        "                 ),\n",
        "                 class_freq=None,\n",
        "                 train_num=None):\n",
        "        super(ResampleLoss, self).__init__()\n",
        "\n",
        "        assert (use_sigmoid is True) or (partial is False)\n",
        "        self.use_sigmoid = use_sigmoid\n",
        "        self.partial = partial\n",
        "        self.loss_weight = loss_weight\n",
        "        self.reduction = reduction\n",
        "        if self.use_sigmoid:\n",
        "            if self.partial:\n",
        "                self.cls_criterion = partial_cross_entropy\n",
        "            else:\n",
        "                self.cls_criterion = binary_cross_entropy\n",
        "        else:\n",
        "            self.cls_criterion = cross_entropy\n",
        "\n",
        "        # reweighting function\n",
        "        self.reweight_func = reweight_func\n",
        "\n",
        "        # normalization (optional)\n",
        "        self.weight_norm = weight_norm\n",
        "\n",
        "        # focal loss params\n",
        "        self.focal = focal['focal']\n",
        "        self.gamma = focal['gamma']\n",
        "        self.alpha = focal['alpha'] # change to alpha\n",
        "\n",
        "        # mapping function params\n",
        "        self.map_alpha = map_param['alpha']\n",
        "        self.map_beta = map_param['beta']\n",
        "        self.map_gamma = map_param['gamma']\n",
        "\n",
        "        # CB loss params (optional)\n",
        "        self.CB_beta = CB_loss['CB_beta']\n",
        "        self.CB_mode = CB_loss['CB_mode']\n",
        "\n",
        "        self.class_freq = torch.from_numpy(np.asarray(class_freq)).float().cuda()\n",
        "        self.num_classes = self.class_freq.shape[0]\n",
        "        self.train_num = train_num # only used to be divided by class_freq\n",
        "        # regularization params\n",
        "        self.logit_reg = logit_reg\n",
        "        self.neg_scale = logit_reg[\n",
        "            'neg_scale'] if 'neg_scale' in logit_reg else 1.0\n",
        "        init_bias = logit_reg['init_bias'] if 'init_bias' in logit_reg else 0.0\n",
        "        self.init_bias = - torch.log(\n",
        "            self.train_num / self.class_freq - 1) * init_bias ########################## bug fixed https://github.com/wutong16/DistributionBalancedLoss/issues/8\n",
        "\n",
        "        self.freq_inv = torch.ones(self.class_freq.shape).cuda() / self.class_freq\n",
        "        self.propotion_inv = self.train_num / self.class_freq\n",
        "\n",
        "    def forward(self,\n",
        "                cls_score_,\n",
        "                label,\n",
        "                weight=None,\n",
        "                avg_factor=None,\n",
        "                reduction_override=None,\n",
        "                **kwargs):\n",
        "\n",
        "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
        "        reduction = (\n",
        "            reduction_override if reduction_override else self.reduction)\n",
        "\n",
        "        weight = self.reweight_functions(label)\n",
        "        cls_score=cls_score_.clone()\n",
        "\n",
        "        cls_score, weight = self.logit_reg_functions(label.float(), cls_score, weight)\n",
        "\n",
        "        if self.focal:\n",
        "            logpt = self.cls_criterion(\n",
        "                cls_score.clone(), label, weight=None, reduction='none',\n",
        "                avg_factor=avg_factor)\n",
        "            # pt is sigmoid(logit) for pos or sigmoid(-logit) for neg\n",
        "            pt = torch.exp(-logpt)\n",
        "            wtloss = self.cls_criterion(\n",
        "                cls_score, label.float(), weight=weight, reduction='none')\n",
        "            alpha_t = torch.where(label==1, self.alpha, 1-self.alpha)\n",
        "            loss = alpha_t * ((1 - pt) ** self.gamma) * wtloss ####################### balance_param should be a tensor\n",
        "            loss = reduce_loss(loss, reduction)             ############################ add reduction\n",
        "        else:\n",
        "            loss = self.cls_criterion(cls_score, label.float(), weight,\n",
        "                                      reduction=reduction)\n",
        "\n",
        "        loss = self.loss_weight * loss\n",
        "        return loss\n",
        "\n",
        "    def reweight_functions(self, label):\n",
        "        if self.reweight_func is None:\n",
        "            return None\n",
        "        elif self.reweight_func in ['inv', 'sqrt_inv']:\n",
        "            weight = self.RW_weight(label.float())\n",
        "        elif self.reweight_func in 'rebalance':\n",
        "            weight = self.rebalance_weight(label.float())\n",
        "        elif self.reweight_func in 'CB':\n",
        "            weight = self.CB_weight(label.float())\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        if self.weight_norm is not None:\n",
        "            if 'by_instance' in self.weight_norm:\n",
        "                max_by_instance, _ = torch.max(weight, dim=-1, keepdim=True)\n",
        "                weight = weight / max_by_instance\n",
        "            elif 'by_batch' in self.weight_norm:\n",
        "                weight = weight / torch.max(weight)\n",
        "\n",
        "        return weight\n",
        "\n",
        "    def logit_reg_functions(self, labels, logits, weight=None): \n",
        "        if not self.logit_reg:\n",
        "            return logits, weight\n",
        "        if 'init_bias' in self.logit_reg:\n",
        "            logits += self.init_bias\n",
        "        if 'neg_scale' in self.logit_reg:\n",
        "            logits = logits * (1 - labels) * self.neg_scale  + logits * labels\n",
        "            if weight is not None:\n",
        "                weight = weight / self.neg_scale * (1 - labels) + weight * labels\n",
        "        return logits, weight\n",
        "\n",
        "    def rebalance_weight(self, gt_labels):\n",
        "        repeat_rate = torch.sum( gt_labels.float() * self.freq_inv, dim=1, keepdim=True)\n",
        "        pos_weight = self.freq_inv.clone().detach().unsqueeze(0) / repeat_rate\n",
        "        # pos and neg are equally treated\n",
        "        weight = torch.sigmoid(self.map_beta * (pos_weight - self.map_gamma)) + self.map_alpha\n",
        "        return weight\n",
        "\n",
        "    def CB_weight(self, gt_labels):\n",
        "        if  'by_class' in self.CB_mode:\n",
        "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                     (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n",
        "        elif 'average_n' in self.CB_mode:\n",
        "            avg_n = torch.sum(gt_labels * self.class_freq, dim=1, keepdim=True) / \\\n",
        "                    torch.sum(gt_labels, dim=1, keepdim=True)\n",
        "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                     (1 - torch.pow(self.CB_beta, avg_n)).cuda()\n",
        "        elif 'average_w' in self.CB_mode:\n",
        "            weight_ = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                      (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n",
        "            weight = torch.sum(gt_labels * weight_, dim=1, keepdim=True) / \\\n",
        "                     torch.sum(gt_labels, dim=1, keepdim=True)\n",
        "        elif 'min_n' in self.CB_mode:\n",
        "            min_n, _ = torch.min(gt_labels * self.class_freq +\n",
        "                                 (1 - gt_labels) * 100000, dim=1, keepdim=True)\n",
        "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                     (1 - torch.pow(self.CB_beta, min_n)).cuda()\n",
        "        else:\n",
        "            raise NameError\n",
        "        return weight\n",
        "\n",
        "    def RW_weight(self, gt_labels, by_class=True):\n",
        "        if 'sqrt' in self.reweight_func:\n",
        "            weight = torch.sqrt(self.propotion_inv)\n",
        "        else:\n",
        "            weight = self.propotion_inv\n",
        "        if not by_class:\n",
        "            sum_ = torch.sum(weight * gt_labels, dim=1, keepdim=True)\n",
        "            weight = sum_ / torch.sum(gt_labels, dim=1, keepdim=True)\n",
        "        return weight\n",
        "    \n",
        "\n",
        "def reduce_loss(loss, reduction):\n",
        "    \"\"\"Reduce loss as specified.\n",
        "    Args:\n",
        "        loss (Tensor): Elementwise loss tensor.\n",
        "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
        "    Return:\n",
        "        Tensor: Reduced loss tensor.\n",
        "    \"\"\"\n",
        "    reduction_enum = F._Reduction.get_enum(reduction)\n",
        "    # none: 0, elementwise_mean:1, sum: 2\n",
        "    if reduction_enum == 0:\n",
        "        return loss\n",
        "    elif reduction_enum == 1:\n",
        "        return loss.mean()\n",
        "    elif reduction_enum == 2:\n",
        "        return loss.sum()\n",
        "\n",
        "\n",
        "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
        "    \"\"\"Apply element-wise weight and reduce loss.\n",
        "    Args:\n",
        "        loss (Tensor): Element-wise loss.\n",
        "        weight (Tensor): Element-wise weights.\n",
        "        reduction (str): Same as built-in losses of PyTorch.\n",
        "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
        "    Returns:\n",
        "        Tensor: Processed loss values.\n",
        "    \"\"\"\n",
        "    # if weight is specified, apply element-wise weight\n",
        "    if weight is not None:\n",
        "        loss = loss * weight\n",
        "\n",
        "    # if avg_factor is not specified, just reduce the loss\n",
        "    if avg_factor is None:\n",
        "        loss = reduce_loss(loss, reduction)\n",
        "    else:\n",
        "        # if reduction is mean, then average the loss by avg_factor\n",
        "        if reduction == 'mean':\n",
        "            loss = loss.sum() / avg_factor\n",
        "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
        "        elif reduction != 'none':\n",
        "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
        "    return loss\n",
        "\n",
        "\n",
        "def binary_cross_entropy(pred,\n",
        "                         label,\n",
        "                         weight=None,\n",
        "                         reduction='mean',\n",
        "                         avg_factor=None):\n",
        "\n",
        "    # weighted element-wise losses\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(\n",
        "        pred, label.float(), weight, reduction='none')\n",
        "    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqXZOgHHtWUB"
      },
      "outputs": [],
      "source": [
        "# class PCL_Model_Arch(nn.Module):\n",
        "#     def __init__(self,pre_trained='roberta-large'):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.bert = AutoModel.from_pretrained(pre_trained, output_hidden_states=True)\n",
        "#         output_channel = 16  # number of kernels\n",
        "#         num_classes = 6  # number of targets to predict\n",
        "#         dropout = 0.2  # dropout value\n",
        "#         embedding_dim = 768   # length of embedding dim\n",
        "\n",
        "#         ks = 3  # three conv nets here\n",
        "\n",
        "#         # input_channel = word embeddings at a value of 1; 3 for RGB images\n",
        "#         input_channel = 4  # for single embedding, input_channel = 1\n",
        "\n",
        "#         # [3, 4, 5] = window height\n",
        "#         # padding = padding to account for height of search window\n",
        "\n",
        "#         # 3 convolutional nets\n",
        "#         self.conv1 = nn.Conv2d(input_channel, output_channel, (3, embedding_dim), padding=(2, 0), groups=4)\n",
        "#         self.conv2 = nn.Conv2d(input_channel, output_channel, (4, embedding_dim), padding=(3, 0), groups=4)\n",
        "#         self.conv3 = nn.Conv2d(input_channel, output_channel, (5, embedding_dim), padding=(4, 0), groups=4)\n",
        "\n",
        "#         # apply dropout\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#         # fully connected layer for classification\n",
        "#         # 3x conv nets * output channel\n",
        "#         self.fc1 = nn.Linear(ks * output_channel, num_classes)\n",
        "#         self.softmax = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, text_id, text_mask):\n",
        "#         # get the last 4 layers\n",
        "#         outputs= self.bert(text_id, attention_mask=text_mask)\n",
        "#         # all_layers  = [4, 16, 256, 768]\n",
        "#         hidden_layers = outputs[-1]  # get hidden layers\n",
        "\n",
        "#         hidden_layers = torch.stack(hidden_layers, dim=1)\n",
        "#         x = hidden_layers[:, -4:] \n",
        "#         # x = x.unsqueeze(1)\n",
        "#         # x = torch.mean(x, 0)\n",
        "#         # print(hidden_layers.size())\n",
        "      \n",
        "#         torch.cuda.empty_cache()\n",
        "#         x = [F.relu(self.conv1(x)).squeeze(3), F.relu(self.conv2(x)).squeeze(3), F.relu(self.conv3(x)).squeeze(3)]\n",
        "#         x = [F.dropout(i) for i in x]\n",
        "#         # max-over-time pooling; # (batch, channel_output) * ks\n",
        "#         x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "#         # concat results; (batch, channel_output * ks)\n",
        "#         x = torch.cat(x, 1)\n",
        "#         # add dropout\n",
        "#         x = self.dropout(x)\n",
        "#         # generate logits (batch, target_size)\n",
        "#         logit = self.fc1(x)\n",
        "#         torch.cuda.empty_cache()\n",
        "#         return logit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class PCL_Model_Arch(nn.Module):\n",
        "#     def __init__(self,pre_trained='roberta-base'):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.bert = AutoModel.from_pretrained(pre_trained, output_hidden_states=True)\n",
        "#         output_channel = 16  # number of kernels\n",
        "#         num_classes = 6  # number of targets to predict\n",
        "#         dropout = 0.2  # dropout value\n",
        "#         embedding_dim = 768   # length of embedding dim\n",
        "\n",
        "#         ks = 3  # three conv nets here\n",
        "\n",
        "#         # input_channel = word embeddings at a value of 1; 3 for RGB images\n",
        "#         input_channel = 4  # for single embedding, input_channel = 1\n",
        "\n",
        "#         # [3, 4, 5] = window height\n",
        "#         # padding = padding to account for height of search window\n",
        "\n",
        "#         # 3 convolutional nets\n",
        "#         self.conv1 = nn.Conv2d(input_channel, output_channel, (3, embedding_dim), padding=(2, 0), groups=4)\n",
        "#         self.conv2 = nn.Conv2d(input_channel, output_channel, (4, embedding_dim), padding=(3, 0), groups=4)\n",
        "#         self.conv3 = nn.Conv2d(input_channel, output_channel, (5, embedding_dim), padding=(4, 0), groups=4)\n",
        "\n",
        "#         # apply dropout\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#         # fully connected layer for classification\n",
        "#         # 3x conv nets * output channel\n",
        "#         self.fc1 = nn.Linear(ks * output_channel, num_classes)\n",
        "#         self.softmax = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, text_id, text_mask):\n",
        "#         # get the last 4 layers\n",
        "#         outputs= self.bert(text_id, attention_mask=text_mask)\n",
        "#         # all_layers  = [4, 16, 256, 768]\n",
        "#         hidden_layers = outputs[2]  # get hidden layers\n",
        "\n",
        "#         hidden_layers = torch.stack(hidden_layers, dim=1)\n",
        "#         x = hidden_layers[:, -4:] \n",
        "#         # x = x.unsqueeze(1)\n",
        "#         # x = torch.mean(x, 0)\n",
        "#         # print(hidden_layers.size())\n",
        "      \n",
        "#         torch.cuda.empty_cache()\n",
        "#         x = [F.relu(self.conv1(x)).squeeze(3), F.relu(self.conv2(x)).squeeze(3), F.relu(self.conv3(x)).squeeze(3)]\n",
        "#         x = [F.dropout(i,0.65) for i in x]\n",
        "#         # max-over-time pooling; # (batch, channel_output) * ks\n",
        "#         x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "#         # concat results; (batch, channel_output * ks)\n",
        "#         x = torch.cat(x, 1)\n",
        "#         # add dropout\n",
        "#         x = self.dropout(x)\n",
        "#         # generate logits (batch, target_size)\n",
        "#         logit = self.fc1(x)\n",
        "#         torch.cuda.empty_cache()\n",
        "#         return logit"
      ],
      "metadata": {
        "id": "tu_rhMyIVRJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class PCL_Model_Arch(nn.Module):\n",
        "#     def __init__(self,pre_trained='roberta-base'):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.bert = AutoModel.from_pretrained(pre_trained)\n",
        "#         D_in, H, D_out = 768, 50, 6\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(D_in, H),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.Linear(H, D_out)\n",
        "#         )\n",
        "\n",
        "\n",
        "#     def forward(self, text_id, text_mask):\n",
        "#         # get the last 4 layers\n",
        "#         outputs= self.bert(text_id, attention_mask=text_mask)\n",
        "#         last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "#         # Feed input to classifier to compute logits\n",
        "#         logits = self.classifier(last_hidden_state_cls)\n",
        "#         return logits"
      ],
      "metadata": {
        "id": "ynFvVfCDAp7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedLayerPooling(nn.Module):\n",
        "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
        "        super(WeightedLayerPooling, self).__init__()\n",
        "        self.layer_start = layer_start\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
        "            else nn.Parameter(\n",
        "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
        "            )\n",
        "\n",
        "    def forward(self, features):\n",
        "        ft_all_layers = features\n",
        "\n",
        "        all_layer_embedding = torch.stack(ft_all_layers)\n",
        "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
        "\n",
        "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
        "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
        "\n",
        "        # features.update({'token_embeddings': weighted_average})\n",
        "        return weighted_average"
      ],
      "metadata": {
        "id": "9t1oErRrFm-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCL_Model_Arch(nn.Module):\n",
        "    def __init__(self,pre_trained='roberta-base'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = AutoModel.from_pretrained(pre_trained, output_hidden_states=True)\n",
        "        self.config = AutoConfig.from_pretrained(pre_trained)\n",
        "        layer_start = 9\n",
        "        self.pooler = WeightedLayerPooling(\n",
        "            self.config.num_hidden_layers, \n",
        "            layer_start=layer_start, layer_weights=None\n",
        "        )\n",
        "        self.cls=nn.Linear(self.config.hidden_size, 6)\n",
        "       \n",
        "       \n",
        "\n",
        "\n",
        "    def forward(self, text_id, text_mask):\n",
        "        # get the last 4 layers\n",
        "        outputs= self.bert(text_id, attention_mask=text_mask)\n",
        "        out=outputs[2]\n",
        "        features = self.pooler(out)\n",
        "        sequence_output = features[:, 0]\n",
        "        logits = self.cls(sequence_output)\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        return logits"
      ],
      "metadata": {
        "id": "in_JRPzGFl4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mixout code to combine vanilla network and dropout network\n",
        "import torch\n",
        "from torch.autograd.function import InplaceFunction\n",
        "class Mixout(InplaceFunction):\n",
        "    # target: a weight tensor mixes with a input tensor\n",
        "    # A forward method returns \n",
        "    # [(1 - Bernoulli(1 - p) mask) * target + (Bernoulli(1 - p) mask) * input - p * target]/(1 - p) \n",
        "    # where p is a mix probability of mixout.\n",
        "    # A backward returns the gradient of the forward method.\n",
        "    # Dropout is equivalent to the case of target=None. \n",
        "    # I modified the code of dropout in PyTorch. \n",
        "    @staticmethod\n",
        "    def _make_noise(input):\n",
        "        return input.new().resize_as_(input)\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, ctx, input, target=None, p=0.0, training=False, inplace=False):\n",
        "        if p < 0 or p > 1:\n",
        "            raise ValueError(\"A mix probability of mixout has to be between 0 and 1,\"\n",
        "                             \" but got {}\".format(p))\n",
        "        if target is not None and input.size() != target.size():\n",
        "            raise ValueError(\"A target tensor size must match with a input tensor size {},\"\n",
        "                             \" but got {}\". format(input.size(), target.size()))\n",
        "        ctx.p = p    \n",
        "        ctx.training = training\n",
        "        \n",
        "        if target is None:\n",
        "            target = cls._make_noise(input)\n",
        "            target.fill_(0)\n",
        "        target = target.to(input.device)\n",
        "\n",
        "        if inplace:\n",
        "            ctx.mark_dirty(input)\n",
        "            output = input\n",
        "        else:\n",
        "            output = input.clone()\n",
        "        \n",
        "        if ctx.p == 0 or not ctx.training:\n",
        "            return output\n",
        "        \n",
        "        ctx.noise = cls._make_noise(input)\n",
        "        if len(ctx.noise.size()) == 1:\n",
        "            ctx.noise.bernoulli_(1 - ctx.p)\n",
        "        else:\n",
        "            ctx.noise[0].bernoulli_(1 - ctx.p)\n",
        "            ctx.noise = ctx.noise[0].repeat(input.size()[0], *([1] * (len(input.size())-1)))\n",
        "        ctx.noise.expand_as(input)\n",
        "        \n",
        "        if ctx.p == 1:\n",
        "            output = target.clone()\n",
        "        else:\n",
        "            output = ((1 - ctx.noise) * target + ctx.noise * output - ctx.p * target) / (1 - ctx.p)\n",
        "        return output\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        if ctx.p > 0 and ctx.training:\n",
        "            return grad_output * ctx.noise, None, None, None, None\n",
        "        else:\n",
        "            return grad_output, None, None, None, None\n",
        "\n",
        "def mixout(input, target=None, p=0.0, training=False, inplace=False):\n",
        "    return Mixout.apply(input, target, p, training, inplace)\n"
      ],
      "metadata": {
        "id": "iR44Cl1kx94d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixLinear(torch.nn.Module):\n",
        "    __constants__ = ['bias', 'in_features', 'out_features']\n",
        "    # If target is None, nn.Sequential(nn.Linear(m, n), MixLinear(m', n', p)) \n",
        "    # is equivalent to nn.Sequential(nn.Linear(m, n), nn.Dropout(p), nn.Linear(m', n')).\n",
        "    # If you want to change a dropout layer to a mixout layer, \n",
        "    # you should replace nn.Linear right after nn.Dropout(p) with Mixout(p) \n",
        "    def __init__(self, in_features, out_features, bias=True, target=None, p=0.0):\n",
        "        super(MixLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        self.target = target\n",
        "        self.p = p\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "            \n",
        "    def forward(self, input):\n",
        "        return F.linear(input, mixout(self.weight, self.target, \n",
        "                                      self.p, self.training), self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        type = 'drop' if self.target is None else 'mix' \n",
        "        return '{}={}, in_features={}, out_features={}, bias={}'.format(type+\"out\", self.p,\n",
        "            self.in_features, self.out_features, self.bias is not None)"
      ],
      "metadata": {
        "id": "1sXL0EY_0f9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def  PCL_Model_Arch(nn.Module):\n",
        "#     def __init__(self,pre_trained='vinai/bertweet-base'):\n",
        "#         super().__init__()"
      ],
      "metadata": {
        "id": "puB614zEyCTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfQwNPiRx_6y"
      },
      "outputs": [],
      "source": [
        "# class PCL_Model_Arch(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(PCL_Model_Arch, self).__init__()\n",
        "#         self.bert = AutoModel.from_pretrained('google/canine-c', output_hidden_states=False)\n",
        "#         self.drop = nn.Dropout(p=0.2)\n",
        "#         self.fc = nn.Linear(768, 2)\n",
        "\n",
        "        \n",
        "#         self.softmax = nn.Softmax()\n",
        "\n",
        "#     def forward(self, text_id, text_mask):\n",
        "#         # get the last 4 layers\n",
        "#         outputs= self.bert(text_id, attention_mask=text_mask)\n",
        "#         # all_layers  = [4, 16, 256, 768]\n",
        "#         hidden_layers = outputs[1]  # get hidden layers\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "        \n",
        "#         x = self.drop(hidden_layers)\n",
        "#         torch.cuda.empty_cache()\n",
        "#         # generate logits (batch, target_size)\n",
        "#         logit = self.fc(x)\n",
        "#         torch.cuda.empty_cache()\n",
        "#         return self.softmax(logit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrNzI4PalQgC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class AsymmetricLoss(nn.Module):\n",
        "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
        "        super(AsymmetricLoss, self).__init__()\n",
        "\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.clip = clip\n",
        "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: input logits\n",
        "        y: targets (multi-label binarized vector)\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculating Probabilities\n",
        "        x_sigmoid = torch.sigmoid(x)\n",
        "        xs_pos = x_sigmoid\n",
        "        xs_neg = 1 - x_sigmoid\n",
        "\n",
        "        # Asymmetric Clipping\n",
        "        if self.clip is not None and self.clip > 0:\n",
        "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
        "\n",
        "        # Basic CE calculation\n",
        "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
        "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
        "        loss = los_pos + los_neg\n",
        "\n",
        "        # Asymmetric Focusing\n",
        "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(False)\n",
        "            pt0 = xs_pos * y\n",
        "            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n",
        "            pt = pt0 + pt1\n",
        "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
        "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(True)\n",
        "            loss *= one_sided_w\n",
        "\n",
        "        return -loss.sum()\n",
        "\n",
        "\n",
        "class AsymmetricLossOptimized(nn.Module):\n",
        "    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
        "    favors inplace operations'''\n",
        "\n",
        "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n",
        "        super(AsymmetricLossOptimized, self).__init__()\n",
        "\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.clip = clip\n",
        "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
        "        self.eps = eps\n",
        "\n",
        "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
        "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: input logits\n",
        "        y: targets (multi-label binarized vector)\n",
        "        \"\"\"\n",
        "\n",
        "        self.targets = y\n",
        "        self.anti_targets = 1 - y\n",
        "\n",
        "        # Calculating Probabilities\n",
        "        self.xs_pos = torch.sigmoid(x)\n",
        "        self.xs_neg = 1.0 - self.xs_pos\n",
        "\n",
        "        # Asymmetric Clipping\n",
        "        if self.clip is not None and self.clip > 0:\n",
        "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
        "\n",
        "        # Basic CE calculation\n",
        "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
        "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
        "\n",
        "        # Asymmetric Focusing\n",
        "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(False)\n",
        "            self.xs_pos = self.xs_pos * self.targets\n",
        "            self.xs_neg = self.xs_neg * self.anti_targets\n",
        "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
        "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(True)\n",
        "            self.loss *= self.asymmetric_w\n",
        "\n",
        "        return -self.loss.sum()\n",
        "\n",
        "\n",
        "class ASLSingleLabel(nn.Module):\n",
        "    '''\n",
        "    This loss is intended for single-label classification problems\n",
        "    '''\n",
        "    def __init__(self, gamma_pos=0, gamma_neg=4, eps: float = 0.1, reduction='mean'):\n",
        "        super(ASLSingleLabel, self).__init__()\n",
        "\n",
        "        self.eps = eps\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
        "        self.targets_classes = []\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        '''\n",
        "        \"input\" dimensions: - (batch_size,number_classes)\n",
        "        \"target\" dimensions: - (batch_size)\n",
        "        '''\n",
        "        num_classes = inputs.size()[-1]\n",
        "        log_preds = self.logsoftmax(inputs)\n",
        "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1)\n",
        "\n",
        "        # ASL weights\n",
        "        targets = self.targets_classes\n",
        "        anti_targets = 1 - targets\n",
        "        xs_pos = torch.exp(log_preds)\n",
        "        xs_neg = 1 - xs_pos\n",
        "        xs_pos = xs_pos * targets\n",
        "        xs_neg = xs_neg * anti_targets\n",
        "        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n",
        "                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
        "        log_preds = log_preds * asymmetric_w\n",
        "\n",
        "        if self.eps > 0:  # label smoothing\n",
        "            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n",
        "\n",
        "        # loss calculation\n",
        "        loss = - self.targets_classes.mul(log_preds)\n",
        "\n",
        "        loss = loss.sum(dim=-1)\n",
        "        if self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT2dbUWN294h",
        "outputId": "6f33514e-a480-4a3e-fa34-8b86a20a4e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=ac364dc5bfa5b10020b87ebf0f6b7da1a1357cb2a31370286eaed8f974191c2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuODW43NYhTz"
      },
      "outputs": [],
      "source": [
        "tokenizer= AutoTokenizer.from_pretrained('roberta-base')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX7ly-CxsN-J"
      },
      "outputs": [],
      "source": [
        "y_train=train[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values\n",
        "class_freq=np.sum(y_train,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWil8TgPs9SD",
        "outputId": "92d0e339-8e04-4127-8d96-bfa60f66ca22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "693"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We57e4GvsbuA"
      },
      "outputs": [],
      "source": [
        "# class_freq=class_freq.reshape(6,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEsjzp8cssLQ",
        "outputId": "8bcd9114-4a5f-4249-ce7f-4b494a1d867b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([575., 119.,  20.,   9.,  34.,  76.])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "class_freq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpuXgp2qyFit"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None):\n",
        "    # element-wise losses\n",
        "    if label.size(-1) != pred.size(0):\n",
        "        label = _squeeze_binary_labels(label)\n",
        "\n",
        "    loss = F.cross_entropy(pred, label, reduction='none')\n",
        "\n",
        "    # apply weights and do the reduction\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "    loss = weight_reduce_loss(\n",
        "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtjKnI8dyLkD"
      },
      "outputs": [],
      "source": [
        "def partial_cross_entropy(pred,\n",
        "                          label,\n",
        "                          weight=None,\n",
        "                          reduction='mean',\n",
        "                          avg_factor=None):\n",
        "    if pred.dim() != label.dim():\n",
        "        label, weight = _expand_binary_labels(label, weight, pred.size(-1))\n",
        "\n",
        "    # weighted element-wise losses\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "\n",
        "    mask = label == -1\n",
        "    loss = F.binary_cross_entropy_with_logits(\n",
        "        pred, label.float(), weight, reduction='none')\n",
        "    if mask.sum() > 0:\n",
        "        loss *= (1-mask).float()\n",
        "        avg_factor = (1-mask).float().sum()\n",
        "\n",
        "    # do the reduction for the weighted loss\n",
        "    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0XUHerFY2jd"
      },
      "outputs": [],
      "source": [
        "def criterion(outputs1,  targets):\n",
        "\n",
        "    criterion =AsymmetricLoss()\n",
        "    # print(outputs1)\n",
        "    # criterion=ResampleLoss(reweight_func='rebalance', loss_weight=1.0,\n",
        "    #                          focal=dict(focal=True, alpha=0.5, gamma=2),\n",
        "    #                          logit_reg=dict(init_bias=0.05, neg_scale=2.0),\n",
        "    #                          map_param=dict(alpha=0.1, beta=10.0, gamma=0.05), \n",
        "    #                          class_freq=class_freq, train_num=len(y_train))\n",
        "    \n",
        "    loss = criterion(outputs1, targets)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeh7f_UdC6dh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "\n",
        "class MultilabelBalancedRandomSampler(Sampler):\n",
        "    \"\"\"\n",
        "    MultilabelBalancedRandomSampler: Given a multilabel dataset of length n_samples and\n",
        "    number of classes n_classes, samples from the data with equal probability per class\n",
        "    effectively oversampling minority classes and undersampling majority classes at the\n",
        "    same time. Note that using this sampler does not guarantee that the distribution of\n",
        "    classes in the output samples will be uniform, since the dataset is multilabel and\n",
        "    sampling is based on a single class. This does however guarantee that all classes\n",
        "    will have at least batch_size / n_classes samples as batch_size approaches infinity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, indices=None, class_choice=\"least_sampled\"):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "            labels: a multi-hot encoding numpy array of shape (n_samples, n_classes)\n",
        "            indices: an arbitrary-length 1-dimensional numpy array representing a list\n",
        "            of indices to sample only from\n",
        "            class_choice: a string indicating how class will be selected for every\n",
        "            sample:\n",
        "                \"least_sampled\": class with the least number of sampled labels so far\n",
        "                \"random\": class is chosen uniformly at random\n",
        "                \"cycle\": the sampler cycles through the classes sequentially\n",
        "        \"\"\"\n",
        "        self.labels = labels\n",
        "        self.indices = indices\n",
        "        if self.indices is None:\n",
        "            self.indices = range(len(labels))\n",
        "\n",
        "        self.num_classes = self.labels.shape[1]\n",
        "\n",
        "        # List of lists of example indices per class\n",
        "        self.class_indices = []\n",
        "        for class_ in range(self.num_classes):\n",
        "            lst = np.where(self.labels[:, class_] == 1)[0]\n",
        "            lst = lst[np.isin(lst, self.indices)]\n",
        "            self.class_indices.append(lst)\n",
        "\n",
        "        self.counts = [0] * self.num_classes\n",
        "\n",
        "        assert class_choice in [\"least_sampled\", \"random\", \"cycle\"]\n",
        "        self.class_choice = class_choice\n",
        "        self.current_class = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.count >= len(self.indices):\n",
        "            raise StopIteration\n",
        "        self.count += 1\n",
        "        return self.sample()\n",
        "\n",
        "    def sample(self):\n",
        "        class_ = self.get_class()\n",
        "        class_indices = self.class_indices[class_]\n",
        "        chosen_index = np.random.choice(class_indices)\n",
        "        if self.class_choice == \"least_sampled\":\n",
        "            for class_, indicator in enumerate(self.labels[chosen_index]):\n",
        "                if indicator == 1:\n",
        "                    self.counts[class_] += 1\n",
        "        return chosen_index\n",
        "\n",
        "    def get_class(self):\n",
        "        if self.class_choice == \"random\":\n",
        "            class_ = random.randint(0, self.labels.shape[1] - 1)\n",
        "        elif self.class_choice == \"cycle\":\n",
        "            class_ = self.current_class\n",
        "            self.current_class = (self.current_class + 1) % self.labels.shape[1]\n",
        "        elif self.class_choice == \"least_sampled\":\n",
        "            min_count = self.counts[0]\n",
        "            min_classes = [0]\n",
        "            for class_ in range(1, self.num_classes):\n",
        "                if self.counts[class_] < min_count:\n",
        "                    min_count = self.counts[class_]\n",
        "                    min_classes = [class_]\n",
        "                if self.counts[class_] == min_count:\n",
        "                    min_classes.append(class_)\n",
        "            class_ = np.random.choice(min_classes)\n",
        "        return class_\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRX0b55VaxmS"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\"seed\": 2021,\n",
        "          \"epochs\": 5,\n",
        "          \"model_name\": \"xlnet-base-cased\",\n",
        "          \"train_batch_size\": 16,\n",
        "          \"valid_batch_size\": 64,\n",
        "          \"max_length\": 120,\n",
        "          \"learning_rate\": 1e-4,\n",
        "          \"scheduler\": 'CosineAnnealingLR',\n",
        "          \"min_lr\": 1e-6,\n",
        "          \"T_max\": 500,\n",
        "          \"weight_decay\": 1e-6,\n",
        "          \"n_fold\": 5,\n",
        "          \"n_accumulate\": 1,\n",
        "          \"num_classes\": 1,\n",
        "          \"margin\": 0.5,\n",
        "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4afl1P8LaD07"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:\n",
        "        \n",
        "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
        "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
        "        targets = data['target'].to(device, dtype=torch.long)\n",
        "        # print(text_ids.shape)\n",
        "        \n",
        "        batch_size = text_ids.size(0)\n",
        "        # print(targets)\n",
        "\n",
        "        outputs = model(text_ids, text_mask)\n",
        "        # print(outputs.shape)\n",
        "        \n",
        "        # print(outputs.shape)\n",
        "\n",
        "        \n",
        "        loss = criterion(outputs, targets)\n",
        "        loss = loss / CONFIG['n_accumulate']\n",
        "        loss.backward()\n",
        "    \n",
        "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
        "            optimizer.step()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "                \n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "        \n",
        "        epoch_loss = running_loss / dataset_size\n",
        "        \n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
        "                        LR=optimizer.param_groups[0]['lr'])\n",
        "    gc.collect()\n",
        "    \n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdWI_KWRafLZ"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "    \n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:        \n",
        "        \n",
        "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
        "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
        "        targets = data['target'].to(device, dtype=torch.long)\n",
        "        # print(text_ids.shape)\n",
        "        \n",
        "        batch_size = text_ids.size(0)\n",
        "\n",
        "        outputs = model(text_ids, text_mask)\n",
        "        # outputs = outputs.argmax(dim=1)\n",
        "        \n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "        \n",
        "        epoch_loss = running_loss / dataset_size\n",
        "        \n",
        "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
        "                        LR=optimizer.param_groups[0]['lr'])   \n",
        "    \n",
        "    gc.collect()\n",
        "    \n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1wtBAJJbEou"
      },
      "outputs": [],
      "source": [
        "def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n",
        "    # To automatically log gradients\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "    \n",
        "    start = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_epoch_loss = np.inf\n",
        "    history = defaultdict(list)\n",
        "    \n",
        "    for epoch in range(1, num_epochs + 1): \n",
        "        gc.collect()\n",
        "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
        "                                           dataloader=train_loader, \n",
        "                                           device=CONFIG['device'], epoch=epoch)\n",
        "        \n",
        "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
        "                                         epoch=epoch)\n",
        "    \n",
        "        history['Train Loss'].append(train_epoch_loss)\n",
        "        history['Valid Loss'].append(val_epoch_loss)\n",
        "        \n",
        "       \n",
        "        \n",
        "        # deep copy the model\n",
        "        if val_epoch_loss <= best_epoch_loss:\n",
        "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
        "            best_epoch_loss = val_epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            PATH = f\"/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_large_kim_cnn/Loss-Fold-{fold}.bin\"\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            # Save a model file from the current directory\n",
        "            print(\"Model Saved\")\n",
        "            \n",
        "        print()\n",
        "    \n",
        "    end = time.time()\n",
        "    time_elapsed = end - start\n",
        "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
        "    \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSXiJa2-bRiX"
      },
      "outputs": [],
      "source": [
        "def fetch_scheduler(optimizer):\n",
        "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
        "                                                   eta_min=CONFIG['min_lr'])\n",
        "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
        "                                                             eta_min=CONFIG['min_lr'])\n",
        "    elif CONFIG['scheduler'] == None:\n",
        "        return None\n",
        "        \n",
        "    return scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6nXhK_ObVNe"
      },
      "outputs": [],
      "source": [
        "def prepare_loaders(fold):\n",
        "    displacemnt_list=[0,512,1024,1536,2048,2560,3072,3584,4096,4608,4950]\n",
        "    df_train = train[train.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = train[train.kfold == fold].reset_index(drop=True)\n",
        "# https://github.com/issamemari/pytorch-multilabel-balanced-sampler/blob/master/example.py\n",
        "    sampler = MultilabelBalancedRandomSampler(df_train[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values,class_choice='cycle')\n",
        "    \n",
        "    train_dataset = PCLTrainDataset(df_train, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=displacemnt_list[fold])\n",
        "    valid_dataset = PCLTrainDataset(df_valid, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=displacemnt_list[fold])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
        "                              num_workers=2, shuffle=False, pin_memory=True, drop_last=True,sampler=sampler)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
        "                              num_workers=2, shuffle=False, pin_memory=True)\n",
        "    \n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnUKG2cbOYcK",
        "outputId": "e2b0572d-1795-45ba-f292-5d48b96c5c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (3.0.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install iterative-stratification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS6M9GV6Obb2"
      },
      "outputs": [],
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "0yEQSRqCORZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EovWKseHOf0l"
      },
      "outputs": [],
      "source": [
        "mskf = MultilabelStratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n",
        "for fold, ( _, val_) in enumerate(mskf.split(train, train[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values)):\n",
        "  train.loc[val_ , \"kfold\"] = int(fold)\n",
        "\n",
        "train[\"kfold\"] = train[\"kfold\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOtw3nW-2X58"
      },
      "outputs": [],
      "source": [
        "# del model,train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoQghNKN2Cy2",
        "outputId": "45e1d572-66b7-421d-e8ca-8c26fc19b3a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "4i4SJMb6VOfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCwmBcvPlNaO"
      },
      "source": [
        "http://seekinginference.com/applied_nlp/bert-cnn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlbu5o9uzyWv",
        "outputId": "8adfbeb2-5b10-4706-d50c-1c57816812c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9bdac0ee10>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/arjundussa65/Thesis-2020"
      ],
      "metadata": {
        "id": "XkS6oTvd05Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer_grouped_parameters(\n",
        "    model, model_type, \n",
        "    learning_rate, weight_decay, \n",
        "    layerwise_learning_rate_decay\n",
        "):\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    # initialize lr for task specific layer\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n or \"pooler\" in n],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": learning_rate,\n",
        "        },\n",
        "    ]\n",
        "    # initialize lrs for every layer\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n",
        "    layers.reverse()\n",
        "    lr = learning_rate\n",
        "    for layer in layers:\n",
        "        lr *= layerwise_learning_rate_decay\n",
        "        optimizer_grouped_parameters += [\n",
        "            {\n",
        "                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"lr\": lr,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "                \"lr\": lr,\n",
        "            },\n",
        "        ]\n",
        "    return optimizer_grouped_parameters"
      ],
      "metadata": {
        "id": "CUe-BnM_03SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXC4SsiDbbGR",
        "outputId": "9d48c7c3-5cb7-47ff-80de-435ffc78ae75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Fold: 0 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:22<00:00,  1.50it/s, Epoch=1, LR=9.89e-5, Train_Loss=9.75]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.35it/s, Epoch=1, LR=9.89e-5, Valid_Loss=28.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 28.882273485334657)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=2, LR=9.56e-5, Train_Loss=3.58]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.40it/s, Epoch=2, LR=9.56e-5, Valid_Loss=25.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (28.882273485334657 ---> 25.382804856883535)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=3, LR=9.02e-5, Train_Loss=1.85]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.40it/s, Epoch=3, LR=9.02e-5, Valid_Loss=30.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=4, LR=8.3e-5, Train_Loss=0.953]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.40it/s, Epoch=4, LR=8.3e-5, Valid_Loss=29.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=5, LR=7.43e-5, Train_Loss=1.16]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.38it/s, Epoch=5, LR=7.43e-5, Valid_Loss=28.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 2m 19s\n",
            "Best Loss: 25.3828\n",
            "\n",
            "====== Fold: 1 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:22<00:00,  1.48it/s, Epoch=1, LR=9.89e-5, Train_Loss=9.69]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.43it/s, Epoch=1, LR=9.89e-5, Valid_Loss=28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 27.984928846359253)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=2, LR=9.56e-5, Train_Loss=3.21]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.42it/s, Epoch=2, LR=9.56e-5, Valid_Loss=27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (27.984928846359253 ---> 27.044928709665935)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=3, LR=9.02e-5, Train_Loss=2]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.41it/s, Epoch=3, LR=9.02e-5, Valid_Loss=27.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=4, LR=8.3e-5, Train_Loss=1.05]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.41it/s, Epoch=4, LR=8.3e-5, Valid_Loss=26.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (27.044928709665935 ---> 26.226904523545418)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=5, LR=7.43e-5, Train_Loss=0.612]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.41it/s, Epoch=5, LR=7.43e-5, Valid_Loss=31.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 2m 21s\n",
            "Best Loss: 26.2269\n",
            "\n",
            "====== Fold: 2 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:22<00:00,  1.49it/s, Epoch=1, LR=9.89e-5, Train_Loss=8.47]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.42it/s, Epoch=1, LR=9.89e-5, Valid_Loss=28.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 28.560207195419203)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=2, LR=9.56e-5, Train_Loss=2.98]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.38it/s, Epoch=2, LR=9.56e-5, Valid_Loss=26.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (28.560207195419203 ---> 26.617695256102856)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=3, LR=9.02e-5, Train_Loss=1.59]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.39it/s, Epoch=3, LR=9.02e-5, Valid_Loss=31.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=4, LR=8.3e-5, Train_Loss=0.958]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.39it/s, Epoch=4, LR=8.3e-5, Valid_Loss=30.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=5, LR=7.43e-5, Train_Loss=0.547]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.40it/s, Epoch=5, LR=7.43e-5, Valid_Loss=29.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 2m 18s\n",
            "Best Loss: 26.6177\n",
            "\n",
            "====== Fold: 3 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:22<00:00,  1.49it/s, Epoch=1, LR=9.89e-5, Train_Loss=8.91]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.42it/s, Epoch=1, LR=9.89e-5, Valid_Loss=23.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 23.632144557486335)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=2, LR=9.56e-5, Train_Loss=2.53]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.42it/s, Epoch=2, LR=9.56e-5, Valid_Loss=28.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=3, LR=9.02e-5, Train_Loss=1.81]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.40it/s, Epoch=3, LR=9.02e-5, Valid_Loss=27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=4, LR=8.3e-5, Train_Loss=1.21]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.40it/s, Epoch=4, LR=8.3e-5, Valid_Loss=24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=5, LR=7.43e-5, Train_Loss=0.56]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.40it/s, Epoch=5, LR=7.43e-5, Valid_Loss=29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 2m 15s\n",
            "Best Loss: 23.6321\n",
            "\n",
            "====== Fold: 4 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla K80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:22<00:00,  1.48it/s, Epoch=1, LR=9.89e-5, Train_Loss=9.54]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.45it/s, Epoch=1, LR=9.89e-5, Valid_Loss=29.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (inf ---> 29.7532531765924)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=2, LR=9.56e-5, Train_Loss=3.56]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.43it/s, Epoch=2, LR=9.56e-5, Valid_Loss=28.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (29.7532531765924 ---> 28.150584538777668)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=3, LR=9.02e-5, Train_Loss=1.6]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.43it/s, Epoch=3, LR=9.02e-5, Valid_Loss=29.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.45it/s, Epoch=4, LR=8.3e-5, Train_Loss=0.901]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.44it/s, Epoch=4, LR=8.3e-5, Valid_Loss=27.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Improved (28.150584538777668 ---> 27.327572504679363)\n",
            "Model Saved\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:23<00:00,  1.44it/s, Epoch=5, LR=7.43e-5, Train_Loss=0.477]\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.43it/s, Epoch=5, LR=7.43e-5, Valid_Loss=29.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 0h 2m 21s\n",
            "Best Loss: 27.3276\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for fold in range(0, CONFIG['n_fold']):\n",
        "    print(f\"====== Fold: {fold} ======\")\n",
        "\n",
        "    \n",
        "    # Create Dataloaders\n",
        "    train_loader, valid_loader = prepare_loaders(fold=fold)\n",
        "    \n",
        "    model = PCL_Model_Arch()\n",
        "    # mixout = 0.7\n",
        "    # for name, module in model.named_modules():\n",
        "    #   if name in ['dropout'] and isinstance(module, nn.Dropout):\n",
        "    #       setattr(model, name, nn.Dropout(0))\n",
        "    #   if name in ['classifier'] and isinstance(module, nn.Linear):\n",
        "    #       target_state_dict = module.state_dict()\n",
        "    #       bias = True if module.bias is not None else False\n",
        "    #       new_module = MixLinear(module.in_features, module.out_features, \n",
        "    #                             bias, target_state_dict['weight'], 0.5)\n",
        "    #       new_module.load_state_dict(target_state_dict)\n",
        "    #       setattr(model, name, new_module)\n",
        "\n",
        "    model.to(CONFIG['device'])\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Define Optimizer and Scheduler\n",
        "    layerwise_learning_rate_decay = 0.9\n",
        "    use_bertadam = False\n",
        "    adam_epsilon = 1e-6\n",
        "    grouped_optimizer_params = get_optimizer_grouped_parameters(model, 'bert',CONFIG['learning_rate'], CONFIG['weight_decay'],0.9)\n",
        "    optimizer = AdamW(grouped_optimizer_params, lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'],correct_bias=not use_bertadam,eps=adam_epsilon)\n",
        "    scheduler = fetch_scheduler(optimizer)\n",
        "    \n",
        "    model, history = run_training(model, optimizer, scheduler,\n",
        "                                  device=CONFIG['device'],\n",
        "                                  num_epochs=CONFIG['epochs'],\n",
        "                                  fold=fold)\n",
        "    \n",
        "    \n",
        "    del model, history, train_loader, valid_loader\n",
        "    _ = gc.collect()\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSvm2lLPAf0i"
      },
      "outputs": [],
      "source": [
        "test.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujNNMM9HAGtl"
      },
      "outputs": [],
      "source": [
        "valid_dataset = PCLTrainDataset(test, tokenizer=tokenizer, max_length=CONFIG['max_length'],displacemnt=0)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
        "                              num_workers=2, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piuXPwW433gP"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def valid_fn(model, dataloader, device):\n",
        "    model.eval()\n",
        "    \n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    PREDS = []\n",
        "    \n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:\n",
        "        ids = data['text_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['text_mask'].to(device, dtype = torch.long)\n",
        "        \n",
        "        outputs = model(ids, mask)\n",
        "        sig=nn.Sigmoid()\n",
        "        outputs=sig(outputs)\n",
        "        # outputs = outputs.argmax(dim=1)\n",
        "#         print(len(outputs))\n",
        "#         print(len(np.max(outputs.cpu().detach().numpy(),axis=1)))\n",
        "        PREDS.append(outputs.detach().cpu().numpy()) \n",
        "        # print(outputs.detach().cpu().numpy())\n",
        "    \n",
        "    PREDS = np.concatenate(PREDS)\n",
        "    gc.collect()\n",
        "    \n",
        "    return PREDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaxyuvUP3Md0"
      },
      "outputs": [],
      "source": [
        "def inference(model_paths, dataloader, device):\n",
        "    final_preds = []\n",
        "    for i, path in enumerate(model_paths):\n",
        "        model = PCL_Model_Arch()\n",
        "        model.to(CONFIG['device'])\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        \n",
        "        print(f\"Getting predictions for model {i+1}\")\n",
        "        preds = valid_fn(model, dataloader, device)\n",
        "        final_preds.append(preds)\n",
        "    \n",
        "    final_preds = np.array(final_preds)\n",
        "    # print(final_preds)\n",
        "    final_preds = np.mean(final_preds, axis=0)\n",
        "    # print(final_preds)\n",
        "    final_preds[final_preds>=0.5] = 1\n",
        "    final_preds[final_preds<0.5] = 0\n",
        "    # final_preds= np.argmax(final_preds,axis=1)\n",
        "    return final_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "roberta_weighted_no_mizout"
      ],
      "metadata": {
        "id": "agYA7xbyP92f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_large_kim_cnn/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_large_kim_cnn/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_large_kim_cnn/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_large_kim_cnn/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_large_kim_cnn/Loss-Fold-4.bin']\n",
        "# MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_kim_cnn/Loss-Fold-0.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ9xxY42QAY1",
        "outputId": "15c7d32a-b678-4ee2-9bf8-eec9ec4bf8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.07it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.16it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.19it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY9flvPeUtmo",
        "outputId": "6a4b7f07-6802-4c7c-ed2e-86e42a37c1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.79      0.99      0.88       138\n",
            "              irony       0.29      0.81      0.43        36\n",
            "             satire       0.33      0.40      0.36         5\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.09      0.50      0.16         6\n",
            "rhetorical_question       0.60      0.84      0.70        25\n",
            "\n",
            "          micro avg       0.55      0.91      0.69       211\n",
            "          macro avg       0.35      0.59      0.42       211\n",
            "       weighted avg       0.65      0.91      0.74       211\n",
            "        samples avg       0.59      0.92      0.69       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX5-ZI9HQAjF",
        "outputId": "10b1123f-91f8-4161-d26d-743693c0b53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.79      0.99      0.88       138\n",
            "              irony       0.29      0.81      0.43        36\n",
            "             satire       0.33      0.40      0.36         5\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.09      0.50      0.16         6\n",
            "rhetorical_question       0.60      0.84      0.70        25\n",
            "\n",
            "          micro avg       0.55      0.91      0.69       211\n",
            "          macro avg       0.35      0.59      0.42       211\n",
            "       weighted avg       0.65      0.91      0.74       211\n",
            "        samples avg       0.59      0.92      0.69       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "roberta_WeightedLayerPooling asymmetric loss with mixout"
      ],
      "metadata": {
        "id": "1NS7S8wWH9qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ec2KtCwjIAdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_WeightedLayerPooling/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_WeightedLayerPooling/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_WeightedLayerPooling/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_WeightedLayerPooling/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_WeightedLayerPooling/Loss-Fold-4.bin']\n",
        "# MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_kim_cnn/Loss-Fold-0.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZIIv4UdICOI",
        "outputId": "a4319fa2-eb49-429c-dc39-5405efcbc368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.00it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.16it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHEoIo3bIDr9",
        "outputId": "64701abc-08e0-4849-f04a-06721ee63eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.79      1.00      0.88       138\n",
            "              irony       0.25      0.86      0.38        36\n",
            "             satire       0.12      0.20      0.15         5\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.05      0.33      0.09         6\n",
            "rhetorical_question       0.58      0.88      0.70        25\n",
            "\n",
            "          micro avg       0.51      0.92      0.65       211\n",
            "          macro avg       0.30      0.55      0.37       211\n",
            "       weighted avg       0.63      0.92      0.73       211\n",
            "        samples avg       0.54      0.94      0.66       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "roberta_mixout_no_kim"
      ],
      "metadata": {
        "id": "uA7O1txMBVVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mix_out_no_kim/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mix_out_no_kim/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mix_out_no_kim/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mix_out_no_kim/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mix_out_no_kim/Loss-Fold-4.bin']\n",
        "# MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_kim_cnn/Loss-Fold-0.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oX13BPFBYX9",
        "outputId": "fa01f95e-4090-4b00-bc26-f7da14a6cf31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.19it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.24it/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0-aJpn_Bvj3",
        "outputId": "c2edfbb2-2a76-4d15-f3af-5aed1db2a4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.79      1.00      0.88       138\n",
            "              irony       0.25      0.81      0.38        36\n",
            "             satire       0.25      0.20      0.22         5\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.04      0.67      0.07         6\n",
            "rhetorical_question       0.35      0.96      0.52        25\n",
            "\n",
            "          micro avg       0.41      0.93      0.57       211\n",
            "          macro avg       0.28      0.61      0.35       211\n",
            "       weighted avg       0.61      0.93      0.71       211\n",
            "        samples avg       0.43      0.94      0.57       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "roberta+ kim+mixout"
      ],
      "metadata": {
        "id": "1blE6RoE6uyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mixout/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mixout/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mixout/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mixout/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/roberta_mixout/Loss-Fold-4.bin']\n",
        "# MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_kim_cnn/Loss-Fold-0.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSDqvgaY6x0f",
        "outputId": "24a9292d-8a84-46fc-e4d6-93944dcf8ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.26s/it]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "id": "FF1o58zYBacN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG26yDzplxe6",
        "outputId": "62c57371-4d88-4aa7-8ccd-94c65b01c727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.79      0.99      0.88       138\n",
            "              irony       0.37      0.72      0.49        36\n",
            "             satire       0.20      0.40      0.27         5\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.07      0.50      0.12         6\n",
            "rhetorical_question       0.53      0.96      0.69        25\n",
            "\n",
            "          micro avg       0.56      0.91      0.69       211\n",
            "          macro avg       0.33      0.60      0.41       211\n",
            "       weighted avg       0.65      0.91      0.75       211\n",
            "        samples avg       0.61      0.92      0.70       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "recall loss cycle -4:"
      ],
      "metadata": {
        "id": "0MrofCCDlmbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_recall_middle_layer_dropout/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_recall_middle_layer_dropout/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_recall_middle_layer_dropout/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_recall_middle_layer_dropout/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_recall_middle_layer_dropout/Loss-Fold-4.bin']\n",
        "# MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_kim_cnn/Loss-Fold-0.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB3Xc__nlycm",
        "outputId": "bfa0d4f1-bde0-4544-8a1c-b5eb5e85d4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgtig_OGlztD",
        "outputId": "d51e17db-7f23-4c2a-c28b-7ce0b70f21c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.79      1.00      0.88       138\n",
            "              irony       0.25      0.03      0.05        36\n",
            "             satire       0.02      0.80      0.05         5\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.00      0.00      0.00         6\n",
            "rhetorical_question       0.14      1.00      0.25        25\n",
            "\n",
            "          micro avg       0.32      0.80      0.46       211\n",
            "          macro avg       0.20      0.47      0.21       211\n",
            "       weighted avg       0.58      0.80      0.62       211\n",
            "        samples avg       0.32      0.79      0.45       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "f1 loss cycle sampler betr -7:-3"
      ],
      "metadata": {
        "id": "7XVbq0GoVxgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_f1_middle_layer_dropout/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_f1_middle_layer_dropout/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_f1_middle_layer_dropout/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_f1_middle_layer_dropout/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_f1_middle_layer_dropout/Loss-Fold-4.bin']\n",
        "# MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_kim_cnn/Loss-Fold-0.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BepxqmxlVw2R",
        "outputId": "4f37ec8d-81ce-461d-d7f1-5fe4b3ba6632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.10s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Vs89N_k8pt",
        "outputId": "e611deb6-1af3-4779-f13c-33d3a7ad8f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.80      0.99      0.88       138\n",
            "              irony       0.64      0.19      0.30        36\n",
            "             satire       0.50      0.20      0.29         5\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.00      0.00      0.00         6\n",
            "rhetorical_question       0.59      0.76      0.67        25\n",
            "\n",
            "          micro avg       0.75      0.78      0.76       211\n",
            "          macro avg       0.42      0.36      0.36       211\n",
            "       weighted avg       0.71      0.78      0.71       211\n",
            "        samples avg       0.77      0.79      0.77       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKHCDN1ipDcA"
      },
      "source": [
        "Assymetric loss random sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmZ3HnfrpP_E",
        "outputId": "ccfe35a7-ab2a-4583-f662-a3c452a8ef64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.12s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.11s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions for model 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\n"
          ]
        }
      ],
      "source": [
        "MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_asymm_extra_dropout_middle_layers/Loss-Fold-0.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_asymm_extra_dropout_middle_layers/Loss-Fold-1.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_asymm_extra_dropout_middle_layers/Loss-Fold-2.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_asymm_extra_dropout_middle_layers/Loss-Fold-3.bin','/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_asymm_extra_dropout_middle_layers/Loss-Fold-4.bin']\n",
        "# MODEL_PATH_2=['/content/drive/MyDrive/ISarcasm/Models_Task_B/bert_tweet_kim_cnn/Loss-Fold-0.bin']\n",
        "preds = inference(MODEL_PATH_2, valid_loader, CONFIG['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWEfZ1fcpRgv",
        "outputId": "6fee9a60-3ab2-4f0c-c167-dc0a04f4e41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.79      1.00      0.88       138\n",
            "              irony       0.23      0.94      0.37        36\n",
            "             satire       0.12      0.40      0.19         5\n",
            "     understatement       0.09      1.00      0.17         1\n",
            "      overstatement       0.06      0.17      0.08         6\n",
            "rhetorical_question       0.43      0.84      0.57        25\n",
            "\n",
            "          micro avg       0.47      0.93      0.63       211\n",
            "          macro avg       0.29      0.73      0.38       211\n",
            "       weighted avg       0.61      0.93      0.72       211\n",
            "        samples avg       0.50      0.95      0.63       211\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, preds,target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT8hjnxbbfJq"
      },
      "source": [
        "## Prepare submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7HICl8MJQf0"
      },
      "outputs": [],
      "source": [
        "!cat task1.txt | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCjziGtxJRif"
      },
      "outputs": [],
      "source": [
        "!cat task2.txt | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZDLUcYZbhYg"
      },
      "outputs": [],
      "source": [
        "!zip submission.zip task1.txt task2.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "bert tweet + roberta_mixout+ 4/8/extra dropout middle layers kim cnn+ f1 macro BCE loss.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}