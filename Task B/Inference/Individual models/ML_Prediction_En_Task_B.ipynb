{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Prediction_En_Task_B.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rM-8vuOcMF3"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "\n",
        "# For data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Utils\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sklearn Imports\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "\n"
      ],
      "metadata": {
        "id": "MVjggdXsVIma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/ISarcasm/Test_dataset/taskA.En.input.csv')"
      ],
      "metadata": {
        "id": "np93eWZNa6Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_svm=pd.read_csv('/content/drive/MyDrive/ISarcasm/ML_task_B_models/svm_model_prediction_ML_FE_4.csv')\n",
        "df_svm_bagging=pd.read_csv('/content/drive/MyDrive/ISarcasm/ML_task_B_models/svm_model_prediction_ML_FE_3.csv')"
      ],
      "metadata": {
        "id": "ljbyiNNHaBeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_svfrom  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, df_svm_bagging[['pred_0', 'pred_1', 'pred_2', 'pred_3', 'pred_4', 'pred_5']],target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVIFdWgGYdK4",
        "outputId": "5bb11b24-2ccf-4214-cd2c-c27e3fe9b635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.14      0.67      0.24       180\n",
            "              irony       0.02      0.45      0.03        20\n",
            "             satire       0.08      0.67      0.14        49\n",
            "     understatement       0.00      0.00      0.00         1\n",
            "      overstatement       0.01      0.30      0.02        10\n",
            "rhetorical_question       0.05      0.64      0.09        11\n",
            "\n",
            "          micro avg       0.07      0.63      0.13       271\n",
            "          macro avg       0.05      0.45      0.08       271\n",
            "       weighted avg       0.11      0.63      0.19       271\n",
            "        samples avg       0.08      0.09      0.08       271\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
        "print(classification_report(test[['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']].values, df_svm[['pred_0', 'pred_1', 'pred_2', 'pred_3', 'pred_4', 'pred_5']],target_names=['sarcasm', 'irony',\n",
        "       'satire', 'understatement', 'overstatement', 'rhetorical_question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnKUCDCRYS3o",
        "outputId": "f1b17aad-d265-45c9-e1f0-86384702eb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "            sarcasm       0.12      0.50      0.20       180\n",
            "              irony       0.02      0.60      0.03        20\n",
            "             satire       0.05      0.73      0.09        49\n",
            "     understatement       0.00      1.00      0.01         1\n",
            "      overstatement       0.01      0.30      0.02        10\n",
            "rhetorical_question       0.05      0.55      0.09        11\n",
            "\n",
            "          micro avg       0.05      0.55      0.09       271\n",
            "          macro avg       0.04      0.61      0.07       271\n",
            "       weighted avg       0.09      0.55      0.15       271\n",
            "        samples avg       0.06      0.08      0.06       271\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}